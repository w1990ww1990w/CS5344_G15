{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import pandas as pd\n",
    "import re\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from datasketch.minhash import MinHash\n",
    "from datasketch.lsh import MinHashLSH\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "#questionFile=sc.textFile(\"Questions.csv\")\n",
    "#answerFile=sc.textFile(\"Answers.csv\")\n",
    "questionFile=sc.textFile(\"test_q.csv\")\n",
    "answerFile=sc.textFile(\"test_a.csv\")\n",
    "tagFile = sc.textFile(\"test_t.csv\")\n",
    "stopWordsFile = sc.textFile(\"stopwords.txt\")\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read sample of question.csv(5%), generate the keywords list to focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "questionWithHeader = questionFile.map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n",
    "header = questionWithHeader.first() #extract header\n",
    "question = questionWithHeader.filter(lambda x: x != header)\n",
    "print(question.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62192\n"
     ]
    }
   ],
   "source": [
    "questionSample=question.sample(False,0.05,81)\n",
    "questionLower = questionSample.map(lambda x: (x[0], re.sub(\"<.*?>\", \" \", (x[5]+x[6]).lower())))\n",
    "def f(x): return re.sub('[^a-zA-Z0-9\\']+', ' ', x)\n",
    "questionLower = questionLower.mapValues(f)\n",
    "def f2(x): return x.split(\" \")\n",
    "questionPairRaw = questionLower.flatMapValues(f2)\n",
    "print(questionPairRaw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "stopWordsList = stopWordsFile.collect()\n",
    "questionPair=questionPairRaw.filter(lambda x: x[1] not in stopWordsList)\n",
    "questionPairfilter=questionPair.filter(lambda x:x[1]!=\"\")\n",
    "questionPairStage1 = questionPairfilter.map(lambda x:(x[0]+\"@\"+x[1],1)).reduceByKey(add)\n",
    "N = len(questionPair.countByKey())\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "questionPairStage2MapPhase = questionPairStage1.map(lambda x:(x[0].split(\"@\")[1],x[0].split(\"@\")[0]+\"=\"+str(x[1])))\n",
    "stage2Map = questionPairStage2MapPhase.countByKey()\n",
    "questionPairStage2 = questionPairStage2MapPhase.map(lambda x:(x[0]+\"@\"+x[1].split(\"=\")[0],(1+np.log(int(x[1].split(\"=\")[1])))*np.log(N/stage2Map.get(x[0]))))\n",
    "questionPairStage3MapPhase = questionPairStage2.map(lambda x:(x[0].split(\"@\")[1],x[0].split(\"@\")[0]+\"=\"+str(x[1])))\n",
    "def f3_1(a,b): return float(a)+float(b.split(\"=\")[1])*float(b.split(\"=\")[1])\n",
    "def f3_2(a,b): return float(a)+float(b)\n",
    "questionPairStage3AggByKey = questionPairStage3MapPhase.aggregateByKey(0.0,f3_1,f3_2)\n",
    "print(questionPairStage3AggByKey.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('4320',\n",
       "  ['bll=0.37277064711218366',\n",
       "   'logic=0.2759487925563063',\n",
       "   'procs=0.17762721066918158',\n",
       "   'validatable=0.17762721066918158',\n",
       "   \"presentation's=0.17762721066918158\",\n",
       "   'objectdatasource=0.17762721066918158',\n",
       "   'preferable=0.17762721066918158']),\n",
       " ('16970',\n",
       "  ['notes=0.40068292564436403',\n",
       "   'offers=0.40068292564436403',\n",
       "   'docs=0.2966112551673511',\n",
       "   'perfect=0.2101776507655334',\n",
       "   'decent=0.2101776507655334',\n",
       "   'clients=0.16233309863451986',\n",
       "   'existing=0.1572333712030617']),\n",
       " ('17020',\n",
       "  ['partitions=0.3632129696362643',\n",
       "   'terabyte=0.21451943091925724',\n",
       "   '2gb=0.21451943091925724',\n",
       "   '2x=0.21451943091925724',\n",
       "   'ram=0.21451943091925724',\n",
       "   'tomorrow=0.21451943091925724',\n",
       "   'divide=0.21451943091925724']),\n",
       " ('20840',\n",
       "  ['dirty=0.46364917386085336',\n",
       "   'reads=0.43748441216684975',\n",
       "   'isolation=0.33285249080393287',\n",
       "   'uncommitted=0.33285249080393287',\n",
       "   'read=0.14503166535203965',\n",
       "   'f=0',\n",
       "   'g=0']),\n",
       " ('22340',\n",
       "  ['wcf=0.2690669399286497',\n",
       "   'firewalls=0.24936138790716952',\n",
       "   'duplex=0.21929937961875334',\n",
       "   'samples=0.17692935865523204',\n",
       "   'irregular=0.11765889963045352',\n",
       "   'ports=0.11765889963045352',\n",
       "   'dual=0.11765889963045352'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionPairStage3AggByKeysqr = questionPairStage3AggByKey.map(lambda x:(x[0],np.sqrt(x[1])))\n",
    "stage3Map = questionPairStage3AggByKeysqr.collectAsMap()\n",
    "questionPairStage3 = questionPairStage3MapPhase.map(lambda x:(x[0],x[1].split(\"=\")[0]+\"=\"+str(float(x[1].split(\"=\")[1])/stage3Map.get(x[0]))))\n",
    "num_keywords = 7\n",
    "def f4_1(a,b):\n",
    "    c=[]\n",
    "    for i in range(num_keywords):\n",
    "        if float(b.split(\"=\")[1])>float(a[i].split(\"=\")[1]):\n",
    "            a[i]=b\n",
    "            break\n",
    "    return a\n",
    "def f4_2(a,b):\n",
    "    for i in range(num_keywords): #b\n",
    "        for j in range(num_keywords): #a\n",
    "            if float(b[i].split(\"=\")[1])>float(a[j].split(\"=\")[1]):\n",
    "                a[j] = b[i]\n",
    "                break\n",
    "    return a\n",
    "questionPairStage4 = questionPairStage3.aggregateByKey([\"a=0\",\"b=0\",\"c=0\",\"d=0\",\"e=0\",\"f=0\",\"g=0\"],f4_1,f4_2)\n",
    "print(questionPairStage4.count())\n",
    "questionPairStage4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "{'connectionstring', 'cooliris', 'offers', 'reports', 'vb', 'processes', 'fgets', 'differences', 'uiwebview', 'binding', 'ray', 'timer', 'ajax', 'cronjob', 'convert', 'alternate', '2000', 'mdi', 'youtube', '400', 'mvc', 'campaign', 'trigger', 'ntfs', 'gpl', 'burn', 'resin', 'axhost', 'oracle', 'character', 'certificate', 'vista', 'concatenate', 'foreign', 'val2', 'xsl', 'url', 'watermark', 'configurations', 'slave', 'arabic', 'menu', 'hour', 'grep', 'ratio', 'growth', 'setout', 'facility', 'httpclient', 'deploying', 'playerwin', 'llvm', 'notes', 'integers', 'vision', 'session', 'orm', 'asynchronous', 'wikipedia', 'simulate', 'newest', '9241', 'berkley', 'front', 'workflow', 'mac', 'varchar', 'factors', 'credit', 'formats', 'practices', 'backbutton', 'restful', 'mm', 'myobj', 'cluster', 'viewdata', 'datagrid', 'abc', 'dirty', 'overused', 'unhandledexceptionmode', 'thumbnail', 'window', 'patterns', 'gdb', 'table1', 'cvs', 'javadoc', 'clojure', 'jtracert', 'addins', 'sprocs', 'preceding', 'hex', 'packet', 'mytype', 'card', 'kernel', 'myform', 'equals', 'div', 'clicked', 'decrypting', 'solace', 'reads', 'versa', 'shortcuts', 'maven', 'sp2', 'language', 'plugin', 'objcontainer', 'logging', 'modelform', 'tag', 'boolean', 'xdocument', 'gcc', 'ror', 'paid', 'algebraic', 'bean', 'installshield', 'exists', 'console', 'formatted', 'arbitrary', 'shutterfly', 'myparam', 'webpage', 'compatible', 'glsl', 'processing', 'channel', 'matrix', 'test', 'increase', 'table2', 'dialogue', 'byte', 'specialfolder', 'dojo', 'scope', \"haskell's\", 'constr', 'component', 'news', 'class', 'duplicates', 'jscocoa', 's2', 'htm', 'singleton', 'iftoday', 'stp', 'unset', 'failures', 'tablecell', 'relevance', 'contents', 'ilist', \"controller's\", 'c1', 'week', 'winform', 'cocoa', 'selectbox', 'richtextbox', 'log4j', 'panel', 'checkbox', 'xml', 'pos', 'ssl', 'benefits', 'latex', 'stringtemplate', 'filter', 'pack', 'vice', 'programming', 'silverlight', 'diagram', 'mygrid', 'ie6', 'relative', 'tells', 'compliant', 'dim', 'valueofelement', 'coverage', 'notification', 'vxworks', 'ob', 'dropbox', 'crystal', 'ascii', 'xunit', 'escape', 'prime', 'fieldname', 'xlib', 'monitoring', 'cut', 'val1', 'datetime', 'endif', 'onactionexecuting', 'indy', 'django', 'replacements', 'commandbutton', 'delphi', 'foundation', 'visualstudio', 'agile', 'lngretval', 'outdir', 'openid', 'controls', 'jbpm', 'finishes', 'registered', 'legend', 'rows', 'mouse', 'estimate', 'displayed', 'rectangle', 'engineexecutionexception', 's1', 'streams', 'dblclick', 'shield', 'concat', 'quantum', 'toad', 'zlib', 'export', 'submit', 'opencv', 'coding', 'css', 'inches', 'nullable', 'linux', '12345', '64', 'quickbooks', 'perl', 'pair', 'watir', 'increment', 'gui', 'cj'}\n"
     ]
    }
   ],
   "source": [
    "def fmap1(x):\n",
    "    output=\"\"\n",
    "    for i in range(num_keywords):\n",
    "        y=x[1][i].split(\"=\")\n",
    "        if float(y[1])>0.4:\n",
    "           output+=y[0]+\";\"\n",
    "    return (\"key\",output)\n",
    "keywordlist=questionPairStage4.map(fmap1).reduceByKey(add)\n",
    "keywords=keywordlist.take(1)\n",
    "words=keywords[0][1][:-1]\n",
    "keywordset=set(words.split(\";\"))\n",
    "print(len(keywordset))\n",
    "print(keywordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn oringinal question pairs to pairs only keep the keywords,get 7 keywords for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25302\n"
     ]
    }
   ],
   "source": [
    "questionLower = question.map(lambda x: (x[0], re.sub(\"<.*?>\", \" \", (x[5]+x[6]).lower())))\n",
    "def f(x): return re.sub('[^a-zA-Z0-9\\']+', ' ', x)\n",
    "questionLower = questionLower.mapValues(f)\n",
    "def f2(x): return x.split(\" \")\n",
    "questionPairRaw = questionLower.flatMapValues(f2)\n",
    "questionPair=questionPairRaw.filter(lambda x: x[1] in keywordset)\n",
    "print(questionPair.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12812\n",
      "[('varchar@80', 11.98072901512991), ('menu@120', 4.071822723979202), ('controls@120', 3.374374461519866), ('card@260', 13.827548207665414), ('language@260', 5.043841666149129)]\n"
     ]
    }
   ],
   "source": [
    "questionPairfilter=questionPair.filter(lambda x:x[1]!=\"\")\n",
    "questionPairStage1 = questionPairfilter.map(lambda x:(x[0]+\"@\"+x[1],1)).reduceByKey(add)\n",
    "N = len(questionPair.countByKey())\n",
    "questionPairStage2MapPhase = questionPairStage1.map(lambda x:(x[0].split(\"@\")[1],x[0].split(\"@\")[0]+\"=\"+str(x[1])))\n",
    "stage2Map = questionPairStage2MapPhase.countByKey()\n",
    "questionPairStage2 = questionPairStage2MapPhase.map(lambda x:(x[0]+\"@\"+x[1].split(\"=\")[0],(1+np.log(int(x[1].split(\"=\")[1])))*np.log(N/stage2Map.get(x[0]))))\n",
    "print(questionPairStage2.count())\n",
    "print(questionPairStage2.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "questionPairStage3MapPhase = questionPairStage2.map(lambda x:(x[0].split(\"@\")[1],x[0].split(\"@\")[0]+\"=\"+str(x[1])))\n",
    "def f3_1(a,b): return float(a)+float(b.split(\"=\")[1])*float(b.split(\"=\")[1])\n",
    "def f3_2(a,b): return float(a)+float(b)\n",
    "questionPairStage3AggByKey = questionPairStage3MapPhase.aggregateByKey(0.0,f3_1,f3_2)\n",
    "questionPairStage3AggByKeysqr = questionPairStage3AggByKey.map(lambda x:(x[0],np.sqrt(x[1])))\n",
    "stage3Map = questionPairStage3AggByKeysqr.collectAsMap()\n",
    "print(stage3Map.get('90'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('330', ['benefits=0.858597929783121', 'b=0', 'c=0', 'd=0', 'e=0', 'f=0', 'g=0']), ('580', ['deploying=0.792509095806917', 'test=0.6098600930240499', 'c=0', 'd=0', 'e=0', 'f=0', 'g=0']), ('2530', ['tag=1.0', 'b=0', 'c=0', 'd=0', 'e=0', 'f=0', 'g=0']), ('2900', ['logging=0.7658192589148438', 'rows=0.6430558783458242', 'c=0', 'd=0', 'e=0', 'f=0', 'g=0']), ('2970', ['practices=0.6558321069911532', 'url=0.47557508058352693', 'c=0', 'd=0', 'e=0', 'f=0', 'g=0'])]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage3 = questionPairStage3MapPhase.map(lambda x:(x[0],x[1].split(\"=\")[0]+\"=\"+str(float(x[1].split(\"=\")[1])/stage3Map.get(x[0]))))\n",
    "\n",
    "num_keywords = 7\n",
    "def f4_1(a,b):\n",
    "    c=[]\n",
    "    for i in range(num_keywords):\n",
    "        if float(b.split(\"=\")[1])>float(a[i].split(\"=\")[1]):\n",
    "            a[i]=b\n",
    "            break\n",
    "    return a\n",
    "def f4_2(a,b):\n",
    "    for i in range(num_keywords): #b\n",
    "        for j in range(num_keywords): #a\n",
    "            if float(b[i].split(\"=\")[1])>float(a[j].split(\"=\")[1]):\n",
    "                a[j] = b[i]\n",
    "                break\n",
    "    return a\n",
    "questionPairStage4 = questionPairStage3.aggregateByKey([\"a=0\",\"b=0\",\"c=0\",\"d=0\",\"e=0\",\"f=0\",\"g=0\"],f4_1,f4_2)\n",
    "print(questionPairStage4.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('330', 'benefits=0.858597929783121:b=0:c=0:d=0:e=0:f=0:g=0'), ('580', 'deploying=0.792509095806917:test=0.6098600930240499:c=0:d=0:e=0:f=0:g=0'), ('2530', 'tag=1.0:b=0:c=0:d=0:e=0:f=0:g=0'), ('2900', 'logging=0.7658192589148438:rows=0.6430558783458242:c=0:d=0:e=0:f=0:g=0'), ('2970', 'practices=0.6558321069911532:url=0.47557508058352693:c=0:d=0:e=0:f=0:g=0')]\n"
     ]
    }
   ],
   "source": [
    "outMap=questionPairStage4.collectAsMap()\n",
    "tagWithHeader = tagFile.map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n",
    "header = tagWithHeader.first() #extract header\n",
    "tag = tagWithHeader.filter(lambda x: x != header)\n",
    "tagLower = tag.map(lambda x: (x[0], x[1].lower()))\n",
    "def f0(x): return re.sub('[^a-zA-Z]+', '', x)\n",
    "tagLower = tagLower.mapValues(f0)\n",
    "tag_1000 = tagLower.map(lambda x: (x[0], x[1]+\"=1000\"))\n",
    "questionPairStage5=questionPairStage4.map(lambda x:(x[0],x[1][0]+\":\"+x[1][1]+\":\"+x[1][2]+\":\"+x[1][3]+\":\"+x[1][4]+\":\"+x[1][5]+\":\"+x[1][6]) )\n",
    "print(questionPairStage5.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionPairStage6=questionPairStage5.union(tag_1000)\n",
    "def f6_1(a,b):\n",
    "    return a+\":\"+b\n",
    "def f6_2(a,b):\n",
    "    return a+b\n",
    "questionPairStage6=questionPairStage6.aggregateByKey(\"\",f6_1,f6_2).map(lambda x: (x[0], x[1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('330', 'benefits=0.858597929783121:b=0:c=0:d=0:e=0:f=0:g=0:c=1000:oop=1000:class=1000:nestedclass=1000'), ('580', 'deploying=0.792509095806917:test=0.6098600930240499:c=0:d=0:e=0:f=0:g=0:sqlserver=1000:sqlserver=1000:deployment=1000:releasemanagement=1000'), ('2970', 'practices=0.6558321069911532:url=0.47557508058352693:c=0:d=0:e=0:f=0:g=0:security=1000:cracking=1000:hijacked=1000'), ('3790', '64=0.7504686006209328:vista=0.3974409339025996:c=0:d=0:e=0:f=0:g=0:windows=1000:bit=1000:wmi=1000'), ('5460', 'tag=1.0:b=0:c=0:d=0:e=0:f=0:g=0:communityserver=1000'), ('5690', 'mvc=0.7973741074052297:convert=0.45246774118980443:c=0:d=0:e=0:f=0:g=0:aspnetmvc=1000:aspnetmvcrouting=1000'), ('6130', 'window=0.8145084617346454:b=0:c=0:d=0:e=0:f=0:g=0:svn=1000:subclipse=1000'), ('7990', 'stringtemplate=0.8878681890649611:class=0.19074254500279683:c=0:d=0:e=0:f=0:g=0:c=1000:net=1000:windowsservices=1000:printing=1000'), ('10670', 'singleton=0.7044793097864777:processing=0.2987256764556073:c=0:d=0:e=0:f=0:g=0:c=1000:net=1000:net=1000:remoting=1000:rpc=1000'), ('10680', 'configurations=1.0:b=0:c=0:d=0:e=0:f=0:g=0:c=1000:linux=1000')]\n"
     ]
    }
   ],
   "source": [
    "print(questionPairStage6.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('330', ('benefits', '0.858597929783121')), ('330', ('c', '1000')), ('330', ('oop', '1000')), ('330', ('class', '1000')), ('330', ('nestedclass', '1000'))]\n"
     ]
    }
   ],
   "source": [
    "questionKeywordAndTag = questionPairStage6.flatMapValues(lambda x: x.split(\":\")).filter(lambda x: float(x[1].split(\"=\")[1]) > 0)\n",
    "questionKeywordAndTag = questionKeywordAndTag.map(lambda x: (x[0], (x[1].split(\"=\")[0], x[1].split(\"=\")[1])))\n",
    "print(questionKeywordAndTag.take(5))\n",
    "\n",
    "questionKeywordAndTagSet = set(questionKeywordAndTag.map(lambda x: x[1][0]).collect())\n",
    "print(len(questionKeywordAndTagSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('330', (45, '0.858597929783121')), ('330', (36, '1000')), ('330', (71, '1000')), ('330', (46, '1000')), ('330', (23, '1000'))]\n"
     ]
    }
   ],
   "source": [
    "partitionNum = 100\n",
    "def hashKeyword(x):\n",
    "    return (int(hashlib.sha1(x.encode()).hexdigest(), 16) % (10 ** 8)) % partitionNum\n",
    "\n",
    "questionHashedKeywordAndTag = questionKeywordAndTag.map(lambda x: (x[0], (hashKeyword(x[1][0]), x[1][1])))\n",
    "print(questionHashedKeywordAndTag.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "questionPartitions = questionHashedKeywordAndTag.map(lambda x: (x[1][0], x[0]))\n",
    "questionPartitions = questionPartitions.groupByKey().sortBy(lambda x: x[0]).map(lambda x: (x[0], set(x[1])))\n",
    "print(questionPartitions.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "[(\"i'm\", 0.2103504541639918), ('spark', 0.35615427839726677), ('order', 0.2103504541639918), ('answers', 0.2103504541639918), ('java', 0.2103504541639918)]\n",
      "[('java', 0.2103504541639918), ('dataset', 0.2103504541639918)]\n"
     ]
    }
   ],
   "source": [
    "newQuestion = sc.wholeTextFiles(\"new_q.txt\")\n",
    "newQuestion = newQuestion.map(lambda x: re.sub('\\n', ' ', x[1]))\n",
    "newQuestion = newQuestion.map(lambda x: re.sub('<.*?>', ' ', x.lower()))\n",
    "newQuestion = newQuestion.map(lambda x: re.sub('[^a-zA-Z0-9\\']+', ' ', x))\n",
    "\n",
    "newQuestionKeywordsStageOne = newQuestion.flatMap(lambda x: x.lower().split(\" \")).filter(lambda x: x != '' and x in questionKeywordAndTagSet).map(lambda x: (x, 1)).reduceByKey(add).map(lambda x: (x[0], 'new=' + str(x[1])))\n",
    "print(newQuestionKeywordsStageOne.count())\n",
    "\n",
    "newQuestionKeywordsStageTwoMap = newQuestionKeywordsStageOne.union(questionPairStage2MapPhase).countByKey()\n",
    "newQuestionKeywordsStageTwo = newQuestionKeywordsStageOne.map(lambda x: (x[0], (1 + np.log(int(x[1].split(\"=\")[1]))) * np.log((N + 1)/newQuestionKeywordsStageTwoMap.get(x[0]))))\n",
    "print(newQuestionKeywordsStageTwo.count())\n",
    "\n",
    "newQuestionKeywordsStageThreeSquare = newQuestionKeywordsStageTwo.map(lambda x: np.square(x[1])).reduce(add)\n",
    "newQuestionKeywordsStageThree = newQuestionKeywordsStageTwo.map(lambda x: (x[0], x[1]/np.sqrt(newQuestionKeywordsStageThreeSquare)))\n",
    "print(newQuestionKeywordsStageThree.take(5))\n",
    "\n",
    "keywordAndTagSet = keywordset.union(tagLower.values().collect())\n",
    "newQuestionKeywordsStageFour = newQuestionKeywordsStageThree.filter(lambda x: x[0] in keywordAndTagSet).sortBy(lambda x: -x[1])\n",
    "print(newQuestionKeywordsStageFour.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('java', 0.2103504541639918), ('dataset', 0.2103504541639918)]\n"
     ]
    }
   ],
   "source": [
    "keywordNum = 10\n",
    "newQuestionKeywords = newQuestionKeywordsStageFour.zipWithIndex().filter(lambda x: x[1] < keywordNum).keys()\n",
    "print(newQuestionKeywords.take(keywordNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 54]\n",
      "532\n"
     ]
    }
   ],
   "source": [
    "newQuestionHashedKeywords = newQuestionKeywords.map(lambda x: hashKeyword(x[0])).collect()\n",
    "print(newQuestionHashedKeywords)\n",
    "bucketQuestions = questionPartitions.filter(lambda x: x[0] in newQuestionHashedKeywords).map(lambda x: x[1]).reduce(lambda a, b: a.union(b))\n",
    "print(len(bucketQuestions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533\n",
      "[('0', [\"i'm using\", 'using spark', 'spark in', 'in order', 'order to', 'to suggest', 'suggest some', 'some answers', 'answers but', 'but i', 'i keep', 'keep getting', 'getting spark', 'spark java', 'java lang', 'lang stackoverflowerror', 'stackoverflowerror when', 'when i', 'i run', 'run my', 'my code', 'code on', 'on a', 'a big', 'big dataset', 'dataset 40k', '40k entries', 'entries when', 'when running', 'running the', 'the code', 'code on', 'on a', 'a small', 'small number', 'number of', 'of entries', 'entries it', 'it works', 'works fine', 'fine though'])]\n",
      "[('0', ['99a3da0112d45b4873e0ae7ce422888d45fa14c8', '129275a321e591df298ff9f03bf9bb6de9088862', 'fee51adbdffb4e8688b46fee7a2769cf8d17e306', '33f6a18ad70cf57665ed9259401237cbff72ec37', '8bed31a162636069e651f82f882b796600f49302', '124cbe7bd236e0cce71ffda5979cc4a0a6d74a62', '2dd5c133a748d53b4e60d995f2f41afb7d4397f1', 'c00ff05b5c1c2aaac10a3668d4493e45db23a0af', 'b028498210c5464e25111ee73315eb73d1568ade', '683c79e433c6646d12e9f1e0a2b18108064dd98f', '517fdc59ebb80af5e080a0e27866a59628480a9b', '6cc6bced9ac7958fef227f2973191cb3a9e274fb', '776de96199cedfbac69c95213b86dbc84f00491f', 'b6830e1b4edb5ffd659db9ef253f93cb5ef6833e', 'e937b7c3b611cb0dbd35c78222155fff06a13e23', '885254333ae84458181b7a3c0cbdb59dc668d392', '3f188391de65c5635e019030066dc0846a7e62d1', '217f669a8fba642b5e7a8468e3cc3dd4c708e016', 'b0bdfbd6a5fa804223b9e8016bf5744924a5fd1f', 'ec15e38d243f173447d43a44ca624c4025cceea4', '3437f69cf25be81de7dfce606a028f5693160fc7', '957b868e5d9858ad32b90e9d170c8f6d7c5e7610', '91eb350085e88d7019f0305cbe568017a6125531', 'c5099b9b8351a8ced458e58eee12ed0e2b68d03a', '50ff0d2d0f3424fd082bdb680ff6f62d0745d85d', 'd35ca49a1fa1375fca391e62d33623eb8bdf3f6b', '491ccfd26cce940dae4523eed2b53b139374474e', 'c24fd37e53b5a136398f97aa9e528d14d56ba954', '956486765be1b0cb6ab8bf87e4338fbc97cc41d0', 'a508f5d5722fbf80ecd434356a3c4cbfc3230c37', 'c0617674c8ca375aaf57dacaf0eb68c1584568b5', '957b868e5d9858ad32b90e9d170c8f6d7c5e7610', '91eb350085e88d7019f0305cbe568017a6125531', '9be02173987c3ade5fdaaf46db78d2a61adcac68', '1ac7c33d52e7b37f98a8c22e557441d513154c43', 'be71753ab132cc8c1497ea4f08700d134a7358fc', 'e31a8f8d969a04b865d37e30b7cf87fc713f346b', '260e0363b6b29f56c2689b09e383592eb21342aa', '813d0f12bcb55abf1c5bc156bacb4375187f794e', 'cae934349fdeacbf85ee2a1164bc2682ca2989ad'])]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# union new question to similar question\n",
    "candidateSimilarQuestions = questionLower.filter(lambda x: x[0] in bucketQuestions)\n",
    "candidateSimilarQuestions = newQuestion.map(lambda x: ['0', x]).union(candidateSimilarQuestions)\n",
    "candidateSimilarQuestions = candidateSimilarQuestions.map(lambda x: (x[0], list(filter(None, x[1].split(\" \")))))\n",
    "print(candidateSimilarQuestions.count())\n",
    "\n",
    "# generate shingles\n",
    "shingleLength = 2\n",
    "candidateQuestionShingles = candidateSimilarQuestions.map(lambda x: (x[0], [\" \".join(x[1][i: i + shingleLength]) for i in range(len(x[1]) - shingleLength + 1)]))\n",
    "print(candidateQuestionShingles.take(1))\n",
    "\n",
    "# hash shingles\n",
    "candidateQuestionHashedShingles = candidateQuestionShingles.map(lambda x: (x[0], [hashlib.sha1(x[1][i].encode()).hexdigest() for i in range(len(x[1]) - 1)]))\n",
    "print(candidateQuestionHashedShingles.take(1))\n",
    "\n",
    "# minhash\n",
    "numPerm = 128\n",
    "m = {}\n",
    "def minhash(x):\n",
    "    m[x[1]] = MinHash(num_perm = numPerm)\n",
    "    for shingle in x[0][1]:\n",
    "        m[x[1]].update(shingle.encode('utf8'))\n",
    "    return (x[0][0], m[x[1]])\n",
    "candidateQuestionMinhash = candidateQuestionHashedShingles.zipWithIndex().map(minhash)\n",
    "\n",
    "newQuestionMinhash = candidateQuestionMinhash.take(1)[0][1]\n",
    "candidateQuestionMinhash = candidateQuestionMinhash.filter(lambda x: x[0] != '0')\n",
    "\n",
    "# lsh\n",
    "threshold = 0.1\n",
    "lsh = MinHashLSH(threshold = threshold, num_perm = numPerm)\n",
    "with lsh.insertion_session() as session:\n",
    "    for x in candidateQuestionMinhash.collect():\n",
    "        session.insert(x[0], x[1])\n",
    "similarQuestions = lsh.query(newQuestionMinhash)\n",
    "print(len(similarQuestions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish reading source files\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "questionDataFrame = sqlContext.read.format(\"csv\").options(header=\"true\").load(\"Questions.csv\")\n",
    "answerDataFrame = sqlContext.read.format(\"csv\").options(header=\"true\").load(\"Answers.csv\")\n",
    "tagDataFrame = sqlContext.read.format(\"csv\").options(header=\"true\").load(\"Tags.csv\")\n",
    "\n",
    "print(\"finish reading source files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Id='185690', Body='\"<p>I\\'m using the <a href=\"\"http://www.componentace.com/zlib_.NET.htm\"\" rel=\"\"nofollow\"\">zlib.NET</a> library to try and inflate files that are compressed by zlib (on a Linux box; perhaps).  Here\\'s what I\\'m doing:</p>  <pre><code>zlib.ZInputStream zinput =     new zlib.ZInputStream(File.Open(path; FileMode.Open; FileAccess.Read));  while (stopByte != (data = zinput.ReadByte())) {   // check data here }  zinput.Close(); </code></pre>  <p>The data bytes match the compressed data bytes; so I must be doing something wrong.</p> \"', Body=\"<p>It appears I made the mistake of assuming all virtual methods were overridden; which wasn't the case.  I was using zlib.ZInputStream.ReadByte(); which is just the inherited Stream.ReadByte(); which doesn't do any inflate.</p>  <p>I used zlib.ZInputStream.Read() instead; and it worked like it should.</p> \", Score='7')]\n",
      "[('185690', ((\"<p>I'm using the <a href='http://www.componentace.com/zlib_.NET.htm' rel='nofollow'>zlib.NET</a> library to try and inflate files that are compressed by zlib (on a Linux box; perhaps).  Here's what I'm doing:</p>  <pre><code>zlib.ZInputStream zinput =     new zlib.ZInputStream(File.Open(path; FileMode.Open; FileAccess.Read));  while (stopByte != (data = zinput.ReadByte())) {   // check data here }  zinput.Close(); </code></pre>  <p>The data bytes match the compressed data bytes; so I must be doing something wrong.</p> \", \"<p>It appears I made the mistake of assuming all virtual methods were overridden; which wasn't the case.  I was using zlib.ZInputStream.ReadByte(); which is just the inherited Stream.ReadByte(); which doesn't do any inflate.</p>  <p>I used zlib.ZInputStream.Read() instead; and it worked like it should.</p> \"), 0.047453537647831036))]\n"
     ]
    }
   ],
   "source": [
    "similarQuestionMinhash = candidateQuestionMinhash.filter(lambda x: x[0] in similarQuestions)\n",
    "similarQuestionJaccardSimilarity = similarQuestionMinhash.map(lambda x: (x[0], newQuestionMinhash.jaccard(x[1]))).collectAsMap()\n",
    "\n",
    "similarQuestionDataFrame = questionDataFrame.filter(questionDataFrame.Id.isin(similarQuestions))\n",
    "similarQuestionDataFrame = similarQuestionDataFrame.join(answerDataFrame, questionDataFrame.Id == answerDataFrame.ParentId).select(questionDataFrame.Id, questionDataFrame.Body, answerDataFrame.Body, answerDataFrame.Score)\n",
    "print(similarQuestionDataFrame.take(1))\n",
    "\n",
    "similarQuestionAnswers = similarQuestionDataFrame.rdd.map(lambda x: (x[0], ((x[1].replace(\"\\\"\\\"\", \"\\'\").replace(\"\\\"\", \"\"), x[2].replace(\"\\\"\\\"\", \"\\'\").replace(\"\\\"\", \"\")), int(x[3]))))\n",
    "similarQuestionAnswerScoreSqrt = similarQuestionAnswers.map(lambda x: (x[0], np.square(x[1][1]))).reduceByKey(add).map(lambda x:(x[0], np.sqrt(x[1])))\n",
    "similarQuestionAnswerScoreSqrt = similarQuestionAnswerScoreSqrt.collectAsMap()\n",
    "\n",
    "similarQuestionAnswerScoreNorm = similarQuestionAnswers.map(lambda x: (x[0], (x[1][0], x[1][1]/similarQuestionAnswerScoreSqrt.get(x[0]))))\n",
    "similarQuestionAnswerScoreNorm = similarQuestionAnswerScoreNorm.map(lambda x: (x[0], (x[1][0], x[1][1] * similarQuestionJaccardSimilarity.get(x[0]))))\n",
    "print(similarQuestionAnswerScoreNorm.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerCount = 10\n",
    "similarQuestionAnswerSorted = similarQuestionAnswerScoreNorm.map(lambda x: x[1]).sortBy(lambda x: -x[1])\n",
    "similarQuestionAnswerTop = similarQuestionAnswerSorted.take(answerCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1\n",
      "Original Question: <p>I'm using the <a href='http://www.componentace.com/zlib_.NET.htm' rel='nofollow'>zlib.NET</a> library to try and inflate files that are compressed by zlib (on a Linux box; perhaps).  Here's what I'm doing:</p>  <pre><code>zlib.ZInputStream zinput =     new zlib.ZInputStream(File.Open(path; FileMode.Open; FileAccess.Read));  while (stopByte != (data = zinput.ReadByte())) {   // check data here }  zinput.Close(); </code></pre>  <p>The data bytes match the compressed data bytes; so I must be doing something wrong.</p> \n",
      "Answer: <p>It appears I made the mistake of assuming all virtual methods were overridden; which wasn't the case.  I was using zlib.ZInputStream.ReadByte(); which is just the inherited Stream.ReadByte(); which doesn't do any inflate.</p>  <p>I used zlib.ZInputStream.Read() instead; and it worked like it should.</p> \n",
      "\n",
      "Rank: 2\n",
      "Original Question: <p>I'm using the <a href='http://www.componentace.com/zlib_.NET.htm' rel='nofollow'>zlib.NET</a> library to try and inflate files that are compressed by zlib (on a Linux box; perhaps).  Here's what I'm doing:</p>  <pre><code>zlib.ZInputStream zinput =     new zlib.ZInputStream(File.Open(path; FileMode.Open; FileAccess.Read));  while (stopByte != (data = zinput.ReadByte())) {   // check data here }  zinput.Close(); </code></pre>  <p>The data bytes match the compressed data bytes; so I must be doing something wrong.</p> \n",
      "Answer: <p>Other than failing to use a 'using' statement to close the stream even in the face of an exception; that looks okay to me. Is the data definitely compressed? Are you able to decompress it with zlib on the linux box?</p>  <p>Having looked at the source code; it's pretty ghastly - a call to <code>int Read(buffer; offset; length)</code> will end up calling its internal <code>int Read()</code> method <code>length</code> times for example. Given that sort of shaky start; I'm not sure I'd trust the code particularly heavily; but I'd have expected it to work at least <em>slightly</em>! Have you tried using <a href='http://www.icsharpcode.net/OpenSource/SharpZipLib/Download.aspx'>SharpZipLib</a>?</p> \n",
      "\n",
      "Rank: 3\n",
      "Original Question: <p>I'm using the <a href='http://www.componentace.com/zlib_.NET.htm' rel='nofollow'>zlib.NET</a> library to try and inflate files that are compressed by zlib (on a Linux box; perhaps).  Here's what I'm doing:</p>  <pre><code>zlib.ZInputStream zinput =     new zlib.ZInputStream(File.Open(path; FileMode.Open; FileAccess.Read));  while (stopByte != (data = zinput.ReadByte())) {   // check data here }  zinput.Close(); </code></pre>  <p>The data bytes match the compressed data bytes; so I must be doing something wrong.</p> \n",
      "Answer: <p>Look at the sample code more closely; it is copying data from a regular Filestream to the ZOutputStream. The decompression must be happening through that layer.</p>  <pre><code>private void decompressFile(string inFile; string outFile) {     System.IO.FileStream outFileStream = new System.IO.FileStream(outFile; System.IO.FileMode.Create);     zlib.ZOutputStream outZStream = new zlib.ZOutputStream(outFileStream);     System.IO.FileStream inFileStream = new System.IO.FileStream(inFile; System.IO.FileMode.Open);\t\t\t     try     {     \tCopyStream(inFileStream; outZStream);     }     finally     {     \toutZStream.Close();     \toutFileStream.Close();     \tinFileStream.Close();     } } </code></pre> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('suggested_answers.csv', 'w') as suggestedAnswers:\n",
    "        writer = csv.writer(suggestedAnswers, delimiter=',')\n",
    "        writer.writerow(['Rank', 'Original Question', 'Answer'])\n",
    "        for idx, questionAnswer in enumerate(similarQuestionAnswerTop):\n",
    "            writer.writerow([idx + 1, questionAnswer[0][0], questionAnswer[0][1]])\n",
    "            \n",
    "for idx, questionAnswer in enumerate(similarQuestionAnswerTop):\n",
    "    print(\"Rank: \" + str(idx + 1))\n",
    "    print(\"Original Question: \" + questionAnswer[0][0])\n",
    "    print(\"Answer: \" + questionAnswer[0][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count top 10 keyword:by tag and by word\n",
    "#(word) title+body->upvotes\n",
    "#(k-items) title+body->upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count top 10 keyword by tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagWords=tagLower.map(lambda x:(x[1],1))\n",
    "tagCounts=tagWords.reduceByKey(add)\n",
    "tagCounts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagSorted = tagCounts.sortBy(lambda x: -x[1])\n",
    "tagSorted.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count top 10 keyword by words\n",
    "questionWords=questionPairStage1.map(lambda x:(x[0].split(\"@\")[1],1))\n",
    "questionCounts=questionWords.reduceByKey(add)\n",
    "questionCounts.take(10)\n",
    "print(questionCounts.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionSorted = questionCounts.sortBy(lambda x: -x[1])\n",
    "questionSorted.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count top 10 keyword by keywords\n",
    "def fmap2(x):\n",
    "    outputs=[]\n",
    "    for i in range(num_keywords):\n",
    "        y=x[1][i].split(\"=\")\n",
    "        if float(y[1])>0.4:\n",
    "           outputs.append((y[0],1))\n",
    "    return (outputs)\n",
    "keywordWords=questionPairStage4.flatMap(fmap2).reduceByKey(add)\n",
    "keywordWords.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordSorted = keywordWords.sortBy(lambda x: -x[1])\n",
    "keywordSorted.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import IndexToString\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSample = data.sample(False,0.10,80)\n",
    "print(dataSample.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Id','Title', 'OwnerUserId', 'CreationDate', 'ClosedDate', 'Code']\n",
    "dataInputs = dataSample.select([column for column in data.columns if column not in drop_list])\n",
    "dataInputs.show(5)\n",
    "dataInputs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Score'\n",
    "def labelfunction(x):\n",
    "    if (int(x)>0):\n",
    "        return \"yes\"\n",
    "    else: return \"no\"\n",
    "udf = UserDefinedFunction(labelfunction, StringType())\n",
    "new_df = dataInputs.select(*[udf(column).alias(name) if column == name else column for column in dataInputs.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.filter(new_df.Score.isNotNull())\n",
    "#new_df = new_df.filter(new_df.Title.isNotNull())\n",
    "new_df = new_df.filter(new_df.Body.isNotNull())\n",
    "new_df.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_value = new_df.agg({\"Score\": \"max\"}).collect()[0][0]\n",
    "#new_df=new_df.withColumn(\"Score\", new_df[\"Score\"].cast(IntegerType()))\n",
    "#print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"Body\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stopWordsList)\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=1000000, minDF=50)\n",
    "label_stringIdx = StringIndexer(inputCol = \"Score\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#label_stringIdx = IndexToString( inputCol=\"Score\", outputCol=\"Label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "#pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "pipelineFit = pipeline.fit(new_df)\n",
    "dataset = pipelineFit.transform(new_df)\n",
    "dataset.show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 1)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "trainingData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=100, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 1) \\\n",
    "    .select(\"Body\",\"Score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 1000, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['label'] == 1) \\\n",
    "    .select(\"Body\",\"Score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 1000, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.5, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Body\",\"Score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 200, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 1) \\\n",
    "    .select(\"Body\",\"Score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 200, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(new_df)\n",
    "dataset = pipelineFit.transform(new_df)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "trainingData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 1) \\\n",
    "    .select(\"Body\",\"Score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 200, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
