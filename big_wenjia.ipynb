{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf \n",
    "import pandas as pd\n",
    "import re\n",
    "from operator import add\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5b2c04ffdd55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(chunk)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#chunk[\"Title\"] = chunk[\"Title\"].map(lambda body: body.replace(\",\", \";\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   2352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;31m# arg is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2354\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         return self._constructor(new_values,\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-5b2c04ffdd55>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(body)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(chunk)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#chunk[\"Title\"] = chunk[\"Title\"].map(lambda body: body.replace(\",\", \";\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/anaconda3/envs/tensorflow/bin/python3.5\"\n",
    "os.environ[\"PYTHONHASHSEED\"]=\"123\"\n",
    "data1 = pd.read_csv(\"_Answers.csv\",encoding=\"latin-1\", iterator = True, chunksize=100)\n",
    "# print(data1)\n",
    "# data1.apply(lambda row: row.replace(\"\\n\", \" \"))\n",
    "# data1.to_csv(\"aaa.csv\",encoding='utf-8')\n",
    "# print(data1.iloc[0])\n",
    "hasHeader = False\n",
    "os.remove(\"aaa.csv\")\n",
    "for chunk in data1:\n",
    "    #print(chunk)\n",
    "    chunk[\"Body\"] = chunk[\"Body\"].map(lambda body: body.replace(\"\\n\", \" \"))\n",
    "    chunk[\"Body\"] = chunk[\"Body\"].map(lambda body: body.replace(\",\", \";\"))\n",
    "    #chunk[\"Title\"] = chunk[\"Title\"].map(lambda body: body.replace(\",\", \";\"))\n",
    "    #print(chunk)\n",
    "    #print(chunk[\"Body\"])\n",
    "    if hasHeader:\n",
    "        chunk.to_csv(\"aaa.csv\", mode = \"a\", encoding=\"utf-8\", header=False, index = False)\n",
    "    else:\n",
    "        chunk.to_csv(\"aaa.csv\", mode = \"a\", encoding=\"utf-8\", header=True, index = False)\n",
    "\n",
    "    #break\n",
    "#     print(type(chunk))\n",
    "#     for index, row in chunk.iterrows():\n",
    "#         print(row)\n",
    "#         print(row[\"Body\"])\n",
    "#         chunk.iloc[index][\"Body\"] = chunk.iloc[index][\"Body\"].replace(\"\\n\", \" \")\n",
    "#         print(chunk.iloc[index][\"Body\"])\n",
    "#         break\n",
    "# data1.to_csv(\"aaa.csv\",encoding='utf-8')\n",
    "# data1 = pd.read_csv(\"aaa.csv\")\n",
    "# data1.replace('\\n','')\n",
    "# data1.to_csv(\"bbb.csv\")\n",
    "print(\"finish\")\n",
    "#df = spark.read.csv(\"aaa.csv\",header=True);\n",
    "#print(df.take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "questionFile=sc.textFile(\"Questions.csv\")\n",
    "answerFile=sc.textFile(\"Answers.csv\")\n",
    "tagFile = sc.textFile(\"Tags.csv\")\n",
    "stopWordsFile = sc.textFile(\"stopwords.txt\")\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['80', '26', '2008-08-01T13:57:07Z', '', '26', 'SQLStatement.execute() - multiple queries in one statement', '\"<p>I\\'ve written a database generation script in <a href=\"\"http://en.wikipedia.org/wiki/SQL\"\">SQL</a> and want to execute it in my <a href=\"\"http://en.wikipedia.org/wiki/Adobe_Integrated_Runtime\"\">Adobe AIR</a> application:</p>  <pre><code>Create Table tRole (       roleID integer Primary Key       ;roleName varchar(40) ); Create Table tFile (     fileID integer Primary Key     ;fileName varchar(50)     ;fileDescription varchar(500)     ;thumbnailID integer     ;fileFormatID integer     ;categoryID integer     ;isFavorite boolean     ;dateAdded date     ;globalAccessCount integer     ;lastAccessTime date     ;downloadComplete boolean     ;isNew boolean     ;isSpotlight boolean     ;duration varchar(30) ); Create Table tCategory (     categoryID integer Primary Key     ;categoryName varchar(50)     ;parent_categoryID integer ); ... </code></pre>  <p>I execute this in Adobe AIR using the following methods:</p>  <pre><code>public static function RunSqlFromFile(fileName:String):void {     var file:File = File.applicationDirectory.resolvePath(fileName);     var stream:FileStream = new FileStream();     stream.open(file; FileMode.READ)     var strSql:String = stream.readUTFBytes(stream.bytesAvailable);     NonQuery(strSql); }  public static function NonQuery(strSQL:String):void {     var sqlConnection:SQLConnection = new SQLConnection();     sqlConnection.open(File.applicationStorageDirectory.resolvePath(DBPATH);     var sqlStatement:SQLStatement = new SQLStatement();     sqlStatement.text = strSQL;     sqlStatement.sqlConnection = sqlConnection;     try     {         sqlStatement.execute();     }     catch (error:SQLError)     {         Alert.show(error.toString());     } } </code></pre>  <p>No errors are generated; however only <code>tRole</code> exists. It seems that it only looks at the first query (up to the semicolon- if I remove it; the query fails). Is there a way to call multiple queries in one statement?</p> \"', 'True'], ['90', '58', '2008-08-01T14:41:24Z', '2012-12-26T03:45:49Z', '144', 'Good branching and merging tutorials for TortoiseSVN?', '\"<p>Are there any really good tutorials explaining <a href=\"\"http://svnbook.red-bean.com/en/1.8/svn.branchmerge.html\"\" rel=\"\"nofollow\"\">branching and merging</a> with Apache Subversion? </p>  <p>All the better if it\\'s specific to TortoiseSVN client.</p> \"', 'False']]\n"
     ]
    }
   ],
   "source": [
    "questionWithHeader = questionFile.map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n",
    "\n",
    "header = questionWithHeader.first() #extract header\n",
    "question = questionWithHeader.filter(lambda x: x != header)\n",
    "print(question.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'The', 'One', 'Two', 'I', 'You', 'Any', 'Me', 'My', 'In', 'It', 'Is']\n"
     ]
    }
   ],
   "source": [
    "stopWordsList = stopWordsFile.collect()\n",
    "print(stopWordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'sqlstatement.execute() - multiple queries in one statement\"<p>i\\'ve written a database generation script in <a href=\"\"http://en.wikipedia.org/wiki/sql\"\">sql</a> and want to execute it in my <a href=\"\"http://en.wikipedia.org/wiki/adobe_integrated_runtime\"\">adobe air</a> application:</p>  <pre><code>create table trole (       roleid integer primary key       ;rolename varchar(40) ); create table tfile (     fileid integer primary key     ;filename varchar(50)     ;filedescription varchar(500)     ;thumbnailid integer     ;fileformatid integer     ;categoryid integer     ;isfavorite boolean     ;dateadded date     ;globalaccesscount integer     ;lastaccesstime date     ;downloadcomplete boolean     ;isnew boolean     ;isspotlight boolean     ;duration varchar(30) ); create table tcategory (     categoryid integer primary key     ;categoryname varchar(50)     ;parent_categoryid integer ); ... </code></pre>  <p>i execute this in adobe air using the following methods:</p>  <pre><code>public static function runsqlfromfile(filename:string):void {     var file:file = file.applicationdirectory.resolvepath(filename);     var stream:filestream = new filestream();     stream.open(file; filemode.read)     var strsql:string = stream.readutfbytes(stream.bytesavailable);     nonquery(strsql); }  public static function nonquery(strsql:string):void {     var sqlconnection:sqlconnection = new sqlconnection();     sqlconnection.open(file.applicationstoragedirectory.resolvepath(dbpath);     var sqlstatement:sqlstatement = new sqlstatement();     sqlstatement.text = strsql;     sqlstatement.sqlconnection = sqlconnection;     try     {         sqlstatement.execute();     }     catch (error:sqlerror)     {         alert.show(error.tostring());     } } </code></pre>  <p>no errors are generated; however only <code>trole</code> exists. it seems that it only looks at the first query (up to the semicolon- if i remove it; the query fails). is there a way to call multiple queries in one statement?</p> \"'), ('90', 'good branching and merging tutorials for tortoisesvn?\"<p>are there any really good tutorials explaining <a href=\"\"http://svnbook.red-bean.com/en/1.8/svn.branchmerge.html\"\" rel=\"\"nofollow\"\">branching and merging</a> with apache subversion? </p>  <p>all the better if it\\'s specific to tortoisesvn client.</p> \"')]\n"
     ]
    }
   ],
   "source": [
    "questionLower = question.map(lambda x: (x[0], (x[5]+x[6]).lower()))\n",
    "print(questionLower.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'sqlstatement execute multiple queries in one statement p i ve written a database generation script in a href http en wikipedia org wiki sql sql a and want to execute it in my a href http en wikipedia org wiki adobe integrated runtime adobe air a application p pre code create table trole roleid integer primary key rolename varchar create table tfile fileid integer primary key filename varchar filedescription varchar thumbnailid integer fileformatid integer categoryid integer isfavorite boolean dateadded date globalaccesscount integer lastaccesstime date downloadcomplete boolean isnew boolean isspotlight boolean duration varchar create table tcategory categoryid integer primary key categoryname varchar parent categoryid integer code pre p i execute this in adobe air using the following methods p pre code public static function runsqlfromfile filename string void var file file file applicationdirectory resolvepath filename var stream filestream new filestream stream open file filemode read var strsql string stream readutfbytes stream bytesavailable nonquery strsql public static function nonquery strsql string void var sqlconnection sqlconnection new sqlconnection sqlconnection open file applicationstoragedirectory resolvepath dbpath var sqlstatement sqlstatement new sqlstatement sqlstatement text strsql sqlstatement sqlconnection sqlconnection try sqlstatement execute catch error sqlerror alert show error tostring code pre p no errors are generated however only code trole code exists it seems that it only looks at the first query up to the semicolon if i remove it the query fails is there a way to call multiple queries in one statement p '), ('90', 'good branching and merging tutorials for tortoisesvn p are there any really good tutorials explaining a href http svnbook red bean com en svn branchmerge html rel nofollow branching and merging a with apache subversion p p all the better if it s specific to tortoisesvn client p ')]\n"
     ]
    }
   ],
   "source": [
    "def f(x): return re.sub('[^a-zA-Z]+', ' ', x)\n",
    "questionLower = questionLower.mapValues(f)\n",
    "print(questionLower.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'sqlstatement'), ('80', 'execute'), ('80', 'multiple'), ('80', 'queries'), ('80', 'in'), ('80', 'one'), ('80', 'statement'), ('80', 'p'), ('80', 'i'), ('80', 've')]\n"
     ]
    }
   ],
   "source": [
    "def f2(x): return x.split(\" \")\n",
    "questionPairRaw = questionLower.flatMapValues(f2)\n",
    "print(questionPairRaw.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'sqlstatement'), ('80', 'execute'), ('80', 'multiple'), ('80', 'queries'), ('80', 'in'), ('80', 'one'), ('80', 'statement'), ('80', 'p'), ('80', 'i'), ('80', 've')]\n"
     ]
    }
   ],
   "source": [
    "questionPair=questionPairRaw.filter(lambda x: x[1] not in stopWordsList)\n",
    "print(questionPair.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'sqlstatement'), ('80', 'execute'), ('80', 'multiple'), ('80', 'queries'), ('80', 'in'), ('80', 'one'), ('80', 'statement'), ('80', 'p'), ('80', 'i'), ('80', 've')]\n"
     ]
    }
   ],
   "source": [
    "questionPairfilter=questionPair.filter(lambda x:x[1]!=\"\")\n",
    "print(questionPairfilter.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80@lastaccesstime', 1), ('80@readutfbytes', 1), ('80@alert', 1), ('80@wiki', 2), ('90@red', 1), ('80@sqlerror', 1), ('90@tutorials', 2), ('80@create', 3), ('80@the', 4), ('90@s', 1)]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage1 = questionPairfilter.map(lambda x:(x[0]+\"@\"+x[1],1)).reduceByKey(add)\n",
    "print(questionPairStage1.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80@lastaccesstime', 1), ('80@readutfbytes', 1), ('80@alert', 1), ('80@wiki', 2), ('90@red', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(questionPairStage1.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "N = len(questionPair.countByKey())\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80@lastaccesstime', 1), ('80@readutfbytes', 1), ('80@alert', 1), ('80@wiki', 2), ('90@red', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(questionPairStage1.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lastaccesstime', '80=1'), ('readutfbytes', '80=1'), ('alert', '80=1'), ('wiki', '80=2'), ('red', '90=1')]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage2MapPhase = questionPairStage1.map(lambda x:(x[0].split(\"@\")[1],x[0].split(\"@\")[0]+\"=\"+str(x[1])))\n",
    "print(questionPairStage2MapPhase.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "stage2Map = questionPairStage2MapPhase.countByKey()\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(stage2Map.get('statement'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lastaccesstime@80', 0.69314718055994529), ('readutfbytes@80', 0.69314718055994529), ('alert@80', 0.69314718055994529), ('wiki@80', 1.1736001944781467), ('red@90', 0.69314718055994529)]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage2 = questionPairStage2MapPhase.map(lambda x:(x[0]+\"@\"+x[1].split(\"=\")[0],(1+np.log(int(x[1].split(\"=\")[1])))*np.log(N/stage2Map.get(x[0]))))\n",
    "print(questionPairStage2.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'lastaccesstime=0.69314718056'), ('80', 'readutfbytes=0.69314718056'), ('80', 'alert=0.69314718056'), ('80', 'wiki=1.17360019448'), ('90', 'red=0.69314718056')]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage3MapPhase = questionPairStage2.map(lambda x:(x[0].split(\"@\")[1],x[0].split(\"@\")[0]+\"=\"+str(x[1])))\n",
    "print(questionPairStage3MapPhase.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 125.1906977852556), ('90', 16.976200374701293)]\n"
     ]
    }
   ],
   "source": [
    "def f3_1(a,b): return float(a)+float(b.split(\"=\")[1])*float(b.split(\"=\")[1])\n",
    "def f3_2(a,b): return float(a)+float(b)\n",
    "questionPairStage3AggByKey = questionPairStage3MapPhase.aggregateByKey(0.0,f3_1,f3_2)\n",
    "print(questionPairStage3AggByKey.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(questionPairStage3AggByKey.lookup('90'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.1202184862821651]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage3AggByKeysqr = questionPairStage3AggByKey.map(lambda x:(x[0],np.sqrt(x[1])))\n",
    "print(questionPairStage3AggByKeysqr.lookup('90'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12021848628\n"
     ]
    }
   ],
   "source": [
    "stage3Map = questionPairStage3AggByKeysqr.collectAsMap()\n",
    "print(stage3Map.get('90'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'lastaccesstime=0.0619497318681'), ('80', 'readutfbytes=0.0619497318681'), ('80', 'alert=0.0619497318681'), ('80', 'wiki=0.104890013849'), ('90', 'red=0.168230685549'), ('80', 'sqlerror=0.0619497318681'), ('90', 'tutorials=0.284839310922'), ('80', 'create=0.130008468578'), ('80', 'the=0.0'), ('90', 's=0.168230685549'), ('90', 'client=0.168230685549'), ('80', 'only=0.104890013849'), ('80', 'at=0.0619497318681'), ('80', 'var=0.161653979002'), ('90', 'en=0.0'), ('80', 'new=0.130008468578'), ('90', 'branchmerge=0.168230685549'), ('80', 'air=0.104890013849'), ('80', 'error=0.104890013849'), ('80', 'globalaccesscount=0.0619497318681'), ('80', 'varchar=0.161653979002'), ('90', 'bean=0.168230685549'), ('80', 'generated=0.0619497318681'), ('80', 'fileid=0.0619497318681'), ('90', 'svn=0.168230685549'), ('90', 'explaining=0.168230685549'), ('80', 'void=0.104890013849'), ('80', 'sqlconnection=0.172948750559'), ('80', 'parent=0.0619497318681'), ('80', 'to=0.0'), ('80', 'execute=0.14783029583'), ('80', 'in=0.161653979002'), ('80', 'way=0.0619497318681'), ('80', 'public=0.104890013849'), ('80', 'date=0.104890013849'), ('80', 'exists=0.0619497318681'), ('80', 'are=0.0'), ('90', 'merging=0.284839310922'), ('90', 'apache=0.168230685549'), ('90', 'href=0.0'), ('80', 'en=0.0'), ('90', 'better=0.168230685549'), ('80', 'applicationstoragedirectory=0.0619497318681'), ('80', 'isfavorite=0.0619497318681'), ('80', 'resolvepath=0.104890013849'), ('80', 'this=0.0619497318681'), ('80', 'methods=0.0619497318681'), ('80', 'adobe=0.130008468578'), ('80', 'thumbnailid=0.0619497318681'), ('80', 'isspotlight=0.0619497318681'), ('80', 'try=0.0619497318681'), ('80', 'tostring=0.0619497318681'), ('80', 'my=0.0619497318681'), ('90', 'com=0.168230685549'), ('80', 'function=0.104890013849'), ('90', 'all=0.168230685549'), ('80', 'catch=0.0619497318681'), ('80', 'script=0.0619497318681'), ('80', 'key=0.130008468578'), ('90', 'the=0.0'), ('80', 'text=0.0619497318681'), ('80', 'href=0.0'), ('80', 'primary=0.130008468578'), ('80', 'read=0.0619497318681'), ('90', 'if=0.0'), ('90', 'p=0.0'), ('80', 'categoryname=0.0619497318681'), ('80', 'if=0.0'), ('90', 'for=0.168230685549'), ('80', 'runsqlfromfile=0.0619497318681'), ('80', 'want=0.0619497318681'), ('80', 'tcategory=0.0619497318681'), ('80', 'dbpath=0.0619497318681'), ('80', 'fileformatid=0.0619497318681'), ('80', 'seems=0.0619497318681'), ('80', 'and=0.0'), ('80', 'string=0.130008468578'), ('80', 'tfile=0.0619497318681'), ('80', 'up=0.0619497318681'), ('80', 'integer=0.19077057781'), ('80', 'statement=0.104890013849'), ('80', 'no=0.0619497318681'), ('80', 'errors=0.0619497318681'), ('80', 'file=0.161653979002'), ('80', 'rolename=0.0619497318681'), ('80', 'fails=0.0619497318681'), ('80', 'filename=0.130008468578'), ('80', 'however=0.0619497318681'), ('90', 'tortoisesvn=0.284839310922'), ('80', 'http=0.0'), ('80', 'multiple=0.104890013849'), ('80', 'wikipedia=0.104890013849'), ('80', 'remove=0.0619497318681'), ('80', 'trole=0.104890013849'), ('90', 'svnbook=0.168230685549'), ('80', 'code=0.172948750559'), ('80', 'pre=0.14783029583'), ('80', 'boolean=0.14783029583'), ('80', 'there=0.0'), ('80', 'roleid=0.0619497318681'), ('80', 'dateadded=0.0619497318681'), ('90', 'specific=0.168230685549'), ('80', 'table=0.130008468578'), ('80', 'queries=0.104890013849'), ('80', 've=0.0619497318681'), ('90', 'there=0.0'), ('80', 'application=0.0619497318681'), ('90', 'and=0.0'), ('80', 'downloadcomplete=0.0619497318681'), ('90', 'it=0.0'), ('80', 'call=0.0619497318681'), ('90', 'rel=0.168230685549'), ('80', 'first=0.0619497318681'), ('80', 'applicationdirectory=0.0619497318681'), ('80', 'strsql=0.14783029583'), ('80', 'looks=0.0619497318681'), ('80', 'nonquery=0.104890013849'), ('90', 'subversion=0.168230685549'), ('90', 'are=0.0'), ('80', 'following=0.0619497318681'), ('80', 'filedescription=0.0619497318681'), ('80', 'org=0.104890013849'), ('80', 'isnew=0.0619497318681'), ('80', 'sqlstatement=0.182498343841'), ('90', 'any=0.168230685549'), ('80', 'that=0.0619497318681'), ('90', 'html=0.168230685549'), ('80', 'stream=0.14783029583'), ('80', 'categoryid=0.130008468578'), ('80', 'filemode=0.0619497318681'), ('80', 'written=0.0619497318681'), ('80', 'generation=0.0619497318681'), ('90', 'good=0.284839310922'), ('80', 'query=0.104890013849'), ('80', 'bytesavailable=0.0619497318681'), ('80', 'semicolon=0.0619497318681'), ('80', 'sql=0.104890013849'), ('80', 'open=0.104890013849'), ('80', 'filestream=0.104890013849'), ('80', 'it=0.0'), ('80', 'integrated=0.0619497318681'), ('80', 'p=0.0'), ('80', 'is=0.0619497318681'), ('90', 'http=0.0'), ('80', 'one=0.104890013849'), ('90', 'really=0.168230685549'), ('80', 'duration=0.0619497318681'), ('80', 'database=0.0619497318681'), ('90', 'nofollow=0.168230685549'), ('90', 'to=0.0'), ('80', 'static=0.104890013849'), ('90', 'branching=0.284839310922'), ('80', 'using=0.0619497318681'), ('80', 'i=0.130008468578'), ('80', 'show=0.0619497318681'), ('90', 'with=0.168230685549'), ('80', 'runtime=0.0619497318681')]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage3 = questionPairStage3MapPhase.map(lambda x:(x[0],x[1].split(\"=\")[0]+\"=\"+str(float(x[1].split(\"=\")[1])/stage3Map.get(x[0]))))\n",
    "print(questionPairStage3.take(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', ['integer=0.19077057781', 'sqlstatement=0.182498343841', 'in=0.161653979002', 'pre=0.14783029583', 'boolean=0.14783029583', 'strsql=0.14783029583', 'stream=0.14783029583']), ('90', ['tutorials=0.284839310922', 'merging=0.284839310922', 'tortoisesvn=0.284839310922', 'good=0.284839310922', 'branching=0.284839310922', 'svn=0.168230685549', 'explaining=0.168230685549'])]\n"
     ]
    }
   ],
   "source": [
    "num_keywords = 7\n",
    "def f4_1(a,b):\n",
    "    c=[]\n",
    "    for i in range(num_keywords):\n",
    "        if float(b.split(\"=\")[1])>float(a[i].split(\"=\")[1]):\n",
    "            a[i]=b\n",
    "            break\n",
    "    return a\n",
    "def f4_2(a,b):\n",
    "    for i in range(num_keywords): #b\n",
    "        for j in range(num_keywords): #a\n",
    "            if float(b[i].split(\"=\")[1])>float(a[j].split(\"=\")[1]):\n",
    "                a[j] = b[i]\n",
    "                break\n",
    "    return a\n",
    "questionPairStage4 = questionPairStage3.aggregateByKey([\"a=0\",\"b=0\",\"c=0\",\"d=0\",\"e=0\",\"f=0\",\"g=0\"],f4_1,f4_2)\n",
    "print(questionPairStage4.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'90': ['tutorials=0.284839310922', 'merging=0.284839310922', 'tortoisesvn=0.284839310922', 'good=0.284839310922', 'branching=0.284839310922', 'svn=0.168230685549', 'explaining=0.168230685549'], '80': ['integer=0.19077057781', 'sqlstatement=0.182498343841', 'in=0.161653979002', 'pre=0.14783029583', 'boolean=0.14783029583', 'strsql=0.14783029583', 'stream=0.14783029583']}\n"
     ]
    }
   ],
   "source": [
    "outMap=questionPairStage4.collectAsMap()\n",
    "print(outMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['80', 'flex'], ['80', 'actionscript-3']]\n"
     ]
    }
   ],
   "source": [
    "tagWithHeader = tagFile.map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n",
    "header = tagWithHeader.first() #extract header\n",
    "tag = tagWithHeader.filter(lambda x: x != header)\n",
    "print(tag.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'flex'), ('80', 'actionscript-3')]\n"
     ]
    }
   ],
   "source": [
    "tagLower = tag.map(lambda x: (x[0], x[1].lower()))\n",
    "print(tagLower.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'flex'), ('80', 'actionscript')]\n"
     ]
    }
   ],
   "source": [
    "def f0(x): return re.sub('[^a-zA-Z]+', '', x)\n",
    "tagLower = tagLower.mapValues(f0)\n",
    "print(tagLower.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'flex=1000'), ('80', 'actionscript=1000')]\n"
     ]
    }
   ],
   "source": [
    "tag_1000 = tagLower.map(lambda x: (x[0], x[1]+\"=1000\"))\n",
    "print(tag_1000.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'integer=0.19077057781:sqlstatement=0.182498343841:in=0.161653979002:pre=0.14783029583:boolean=0.14783029583:strsql=0.14783029583:stream=0.14783029583'), ('90', 'tutorials=0.284839310922:merging=0.284839310922:tortoisesvn=0.284839310922:good=0.284839310922:branching=0.284839310922:svn=0.168230685549:explaining=0.168230685549')]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage5=questionPairStage4.map(lambda x:(x[0],x[1][0]+\":\"+x[1][1]+\":\"+x[1][2]+\":\"+x[1][3]+\":\"+x[1][4]+\":\"+x[1][5]+\":\"+x[1][6]) )\n",
    "print(questionPairStage5.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'integer=0.19077057781:sqlstatement=0.182498343841:in=0.161653979002:pre=0.14783029583:boolean=0.14783029583:strsql=0.14783029583:stream=0.14783029583:flex=1000:actionscript=1000:air=1000')]\n"
     ]
    }
   ],
   "source": [
    "questionPairStage6=questionPairStage5.union(tag_1000)\n",
    "def f6_1(a,b):\n",
    "    return a+\":\"+b\n",
    "def f6_2(a,b):\n",
    "    return a+b\n",
    "questionPairStage6=questionPairStage6.aggregateByKey(\"\",f6_1,f6_2).map(lambda x: (x[0], x[1][1:]))\n",
    "print(questionPairStage6.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('80', 'flex=1000:actionscript=1000:air=1000:integer=0.19077057781:sqlstatement=0.182498343841:in=0.161653979002:pre=0.14783029583'), ('90', 'svn=1000:tortoisesvn=1000:branch=1000:branchingandmerging=1000:tutorials=0.284839310922:merging=0.284839310922:tortoisesvn=0.284839310922')]\n"
     ]
    }
   ],
   "source": [
    "def f7(x):\n",
    "    listx= x[1].split(\":\")\n",
    "    listx.sort(key=lambda item:float(item.split(\"=\")[1]),reverse=True)\n",
    "    output=\"\"\n",
    "    for i in range(num_keywords):\n",
    "        output+=\":\"+listx[i]\n",
    "    return (x[0],output)\n",
    "questionPairStage7=questionPairStage6.map(f7).map(lambda x: (x[0], x[1][1:]))\n",
    "print(questionPairStage7.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sqlstatement execute multiple queries in one statement p i ve written a database generation script in a href http en wikipedia org wiki sql sql a and want to execute it in my a href http en wikipedia org wiki adobe integrated runtime adobe air a application p pre code create table trole roleid integer primary key rolename varchar create table tfile fileid integer primary key filename varchar filedescription varchar thumbnailid integer fileformatid integer categoryid integer isfavorite boolean dateadded date globalaccesscount integer lastaccesstime date downloadcomplete boolean isnew boolean isspotlight boolean duration varchar create table tcategory categoryid integer primary key categoryname varchar parent categoryid integer code pre p i execute this in adobe air using the following methods p pre code public static function runsqlfromfile filename string void var file file file applicationdirectory resolvepath filename var stream filestream new filestream stream open file filemode read var strsql string stream readutfbytes stream bytesavailable nonquery strsql public static function nonquery strsql string void var sqlconnection sqlconnection new sqlconnection sqlconnection open file applicationstoragedirectory resolvepath dbpath var sqlstatement sqlstatement new sqlstatement sqlstatement text strsql sqlstatement sqlconnection sqlconnection try sqlstatement execute catch error sqlerror alert show error tostring code pre p no errors are generated however only code trole code exists it seems that it only looks at the first query up to the semicolon if i remove it the query fails is there a way to call multiple queries in one statement p '], ['good branching and merging tutorials for tortoisesvn p are there any really good tutorials explaining a href http svnbook red bean com en svn branchmerge html rel nofollow branching and merging a with apache subversion p p all the better if it s specific to tortoisesvn client p ']]\n",
      "[(10, 'p'), (8, 'integer'), (8, 'a'), (7, 'sqlstatement'), (6, 'sqlconnection'), (6, 'code'), (5, 'the'), (5, 'in'), (5, 'varchar'), (5, 'var')]\n",
      "{'': 32, 'up': 102, 'adobe': 26, 'in': 7, 'and': 30, 'rolename': 87, 'dbpath': 144, 'application': 104, 'remove': 66, 'specific': 110, 'integer': 1, 'rel': 99, 'fails': 100, 'written': 82, 'filename': 27, 'p': 0, 'wikipedia': 62, 'svn': 109, 'boolean': 13, 'lastaccesstime': 140, 'roleid': 101, 'exists': 77, 'readutfbytes': 122, 'try': 134, 'text': 93, 'applicationdirectory': 95, 'however': 83, 'nofollow': 127, 'tfile': 75, 'only': 52, 'execute': 16, 'following': 105, 'duration': 113, 'are': 43, 'filedescription': 146, 'tostring': 123, 'call': 96, 'is': 124, 'org': 38, 'public': 57, 'to': 14, 'table': 18, 'red': 106, 'code': 5, 'for': 120, 'tutorials': 55, 'bytesavailable': 92, 'alert': 70, 'error': 51, 'nonquery': 47, 'one': 61, 'if': 53, 'primary': 29, 'isfavorite': 142, 'fileformatid': 147, 'with': 126, 'my': 63, 'script': 98, 'errors': 108, 'semicolon': 67, 'no': 71, 'bean': 137, 'it': 10, 'integrated': 119, 'show': 78, 'wiki': 60, 'downloadcomplete': 135, 'first': 116, 'runsqlfromfile': 103, 'void': 54, 'dateadded': 136, 'isspotlight': 118, 'i': 19, 'sql': 35, 'href': 23, 've': 131, 'parent': 81, 'better': 80, 'there': 50, 'svnbook': 115, 'applicationstoragedirectory': 94, 'want': 128, 'seems': 121, 'air': 42, 'filemode': 133, 'generation': 130, 'good': 40, 'html': 145, 'sqlstatement': 3, 'file': 11, 'really': 111, 'com': 89, 'this': 91, 'tortoisesvn': 56, 'sqlconnection': 4, 'a': 2, 'subversion': 132, 'database': 73, 'catch': 74, 'tcategory': 117, 'var': 9, 'queries': 48, 'resolvepath': 36, 'isnew': 68, 'stream': 15, 'client': 107, 'string': 24, 'generated': 112, 'filestream': 37, 'looks': 88, 'globalaccesscount': 139, 'categoryid': 25, 'categoryname': 76, 'varchar': 8, 'http': 28, 'branching': 49, 'thumbnailid': 79, 'at': 90, 'function': 59, 'en': 20, 'read': 97, 'create': 22, 'key': 21, 'any': 114, 'branchmerge': 143, 'open': 39, 'methods': 85, 'sqlerror': 72, 'all': 138, 'trole': 45, 'way': 64, 'fileid': 84, 'strsql': 17, 'that': 141, 'explaining': 86, 'pre': 12, 'apache': 129, 'using': 69, 'date': 58, 'statement': 41, 'multiple': 44, 'new': 31, 'merging': 33, 'query': 34, 'static': 46, 'the': 6, 's': 125, 'runtime': 65}\n",
      "Learned topics (as distributions over vocab of 148 words):\n",
      "Topic 0:\n",
      "Topic 1:\n",
      "Topic 2:\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "#Clustring\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON']=\"/anaconda3/envs/tensorflow/bin/python3.5\"\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def document_vector(document):\n",
    "    id = document[1]\n",
    "    counts = defaultdict(int)\n",
    "    for token in document[0]:\n",
    "        if token in vocabulary:\n",
    "            token_id = vocabulary[token]\n",
    "            counts[token_id] += 1\n",
    "    counts = sorted(counts.items())\n",
    "    keys = [x[0] for x in counts]\n",
    "    values = [x[1] for x in counts]\n",
    "    return (id, Vectors.sparse(len(vocabulary), keys, values))\n",
    "#header = questionLower.first()\n",
    "\n",
    "\n",
    "data=questionPairRaw#questionLower\n",
    "documents=questionLower.map(lambda doc:[x for x in doc if len(x)>3])\n",
    "#print(data.take(1))\n",
    "#data = data.map(lambda words:(words[0],list(words[1].split(' '))))\n",
    "#print(data.take(1))\n",
    "data = data.map(lambda words: words[1])#.filter(lambda x: len(x)>3)  \n",
    "\n",
    "#print(data.take(10))\n",
    "data = data.map(lambda words: (words,1))\n",
    "data = data.reduceByKey(add) \n",
    "#print(data.take(10))\n",
    "data=data.map(lambda tuple: (tuple[1], tuple[0])).sortByKey(False)\n",
    "\n",
    "vocabulary = data.map(lambda x: [x[0], x[1]]).map(lambda x: x[1]).zipWithIndex().collectAsMap()\n",
    "inv_voc = {value: key for (key, value) in vocabulary.items()}\n",
    "print(documents.take(5))\n",
    "documents=documents.zipWithIndex().map(document_vector).map(list)\n",
    "print(data.take(10))\n",
    "print(vocabulary)\n",
    "#all docs\n",
    "num_topics=3\n",
    "with open(\"output.txt\", 'w') as f:\n",
    "    lda_model = LDA.train(documents, k=num_topics, maxIterations=10)\n",
    "                    \n",
    "    topic_indices = lda_model.describeTopics(maxTermsPerTopic=10)\n",
    "    \n",
    "    # Output topics. Each is a distribution over words (matching word count vectors)\n",
    "    print(\"Learned topics (as distributions over vocab of \" + str(lda_model.vocabSize())\\\n",
    "      + \" words):\")\n",
    "    topics = lda_model.topicsMatrix()\n",
    "    for topic in range(3):\n",
    "        print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, lda_model.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))\n",
    "\n",
    "        \n",
    "    # Print topics, showing the top-weighted 10 terms for each topic\n",
    "    for i in range(len(topic_indices)):\n",
    "        f.write(\"Topic #{0}\\n\".format(i + 1))\n",
    "        for j in range(len(topic_indices[i][0])):\n",
    "            f.write(\"{0}\\t{1}\\n\".format(inv_voc[topic_indices[i][0][j]] \\\n",
    "                .encode('utf-8'), topic_indices[i][1][j]))\n",
    "            \n",
    "\n",
    "    f.write(\"{0} topics distributed over {1} documents and {2} unique words\\n\"  \\\n",
    "        .format(num_topics, documents.count(), len(vocabulary)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['92', '61', '2008-08-01T14:45:37Z', '90', '13', '\"<p><a href=\"\"http://svnbook.red-bean.com/\"\">Version Control with Subversion</a></p>  <p>A very good resource for source control in general. Not really TortoiseSVN specific; though.</p>\"', 'False'], ['124', '26', '2008-08-01T16:09:47Z', '80', '12', '\"<p>I wound up using this. It is a kind of a hack; but it actually works pretty well. The only thing is you have to be very careful with your semicolons. : D</p>  <pre><code>var strSql:String = stream.readUTFBytes(stream.bytesAvailable);       var i:Number = 0; var strSqlSplit:Array = strSql.split(\"\";\"\"); for (i = 0; i &lt; strSqlSplit.length; i++){     NonQuery(strSqlSplit[i].toString()); } </code></pre> \"', 'True']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answerFileWithHeader = answerFile.map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n",
    "\n",
    "header = answerFileWithHeader.first() \n",
    "answer = answerFileWithHeader.filter(lambda x: x != header)\n",
    "print(answer.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('92', '\"<p><a href=\"\"http://svnbook.red-bean.com/\"\">Version Control with Subversion</a></p>  <p>A very good resource for source control in general. Not really TortoiseSVN specific; though.</p>\"'), ('124', '\"<p>I wound up using this. It is a kind of a hack; but it actually works pretty well. The only thing is you have to be very careful with your semicolons. : D</p>  <pre><code>var strSql:String = stream.readUTFBytes(stream.bytesAvailable);       var i:Number = 0; var strSqlSplit:Array = strSql.split(\"\";\"\"); for (i = 0; i &lt; strSqlSplit.length; i++){     NonQuery(strSqlSplit[i].toString()); } </code></pre> \"')]\n"
     ]
    }
   ],
   "source": [
    "answer=answer.map(lambda x: (x[0],x[5]))\n",
    "print(answer.take(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('92', ' p a href http svnbook red bean com Version Control with Subversion a p p A very good resource for source control in general Not really TortoiseSVN specific though p '), ('124', ' p I wound up using this It is a kind of a hack but it actually works pretty well The only thing is you have to be very careful with your semicolons D p pre code var strSql String stream readUTFBytes stream bytesAvailable var i Number var strSqlSplit Array strSql split for i i lt strSqlSplit length i NonQuery strSqlSplit i toString code pre ')]\n"
     ]
    }
   ],
   "source": [
    "def f(x): return re.sub('[^a-zA-Z]+', ' ', x)\n",
    "answerLower = answer.mapValues(f)\n",
    "print(answerLower.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('92', 'Answer@92 p a href http svnbook red bean com Version Control with Subversion a p p A very good resource for source control in general Not really TortoiseSVN specific though p '), ('124', 'Answer@124 p I wound up using this It is a kind of a hack but it actually works pretty well The only thing is you have to be very careful with your semicolons D p pre code var strSql String stream readUTFBytes stream bytesAvailable var i Number var strSqlSplit Array strSql split for i i lt strSqlSplit length i NonQuery strSqlSplit i toString code pre ')]\n"
     ]
    }
   ],
   "source": [
    "answerwithTag = answerLower.map(lambda x:(x[0],\"Answer@\"+x[0]+x[1]))\n",
    "print(answerwithTag.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('92', 'Answer@92 p a href http svnbook red bean com Version Control with Subversion a p p A very good resource for source control in general Not really TortoiseSVN specific though p '), ('124', 'Answer@124 p I wound up using this It is a kind of a hack but it actually works pretty well The only thing is you have to be very careful with your semicolons D p pre code var strSql String stream readUTFBytes stream bytesAvailable var i Number var strSqlSplit Array strSql split for i i lt strSqlSplit length i NonQuery strSqlSplit i toString code pre ')]\n"
     ]
    }
   ],
   "source": [
    "#(questionid, sum of answers)\n",
    "def sumAsList(x):\n",
    "    list=[]\n",
    "    list.append(x[0])\n",
    "    list.append(x[1])\n",
    "    return list\n",
    "answerSum=answerwithTag.reduceByKey(sumAsList)\n",
    "print(answerSum.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
