{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intend to predict answer upvotes(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "100/100 [==============================] - 154s 2s/step - loss: 0.0184 - mean_squared_error: 0.0184 - mean_absolute_error: 0.0523 - mean_absolute_percentage_error: 1639622.0252 - val_loss: 0.0313 - val_mean_squared_error: 0.0313 - val_mean_absolute_error: 0.0719 - val_mean_absolute_percentage_error: 77.6172\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.03130, saving model to ./checkpoint-01-0.0313.hdf5\n",
      "Epoch 2/24\n",
      "100/100 [==============================] - 50s 501ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - mean_absolute_error: 0.0298 - mean_absolute_percentage_error: 64.3789 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.0539 - val_mean_absolute_percentage_error: 72.3926\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.03130 to 0.02052, saving model to ./checkpoint-02-0.0205.hdf5\n",
      "Epoch 3/24\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - mean_absolute_error: 0.0231 - mean_absolute_percentage_error: 60.4201 - val_loss: 0.0174 - val_mean_squared_error: 0.0174 - val_mean_absolute_error: 0.0477 - val_mean_absolute_percentage_error: 68.8086\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.02052 to 0.01735, saving model to ./checkpoint-03-0.0174.hdf5\n",
      "Epoch 4/24\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - mean_absolute_error: 0.0193 - mean_absolute_percentage_error: 59.0198 - val_loss: 0.0183 - val_mean_squared_error: 0.0183 - val_mean_absolute_error: 0.0532 - val_mean_absolute_percentage_error: 76.1719\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve\n",
      "Epoch 5/24\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - mean_absolute_error: 0.0161 - mean_absolute_percentage_error: 53.0796 - val_loss: 0.0138 - val_mean_squared_error: 0.0138 - val_mean_absolute_error: 0.0445 - val_mean_absolute_percentage_error: 71.4160\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.01735 to 0.01381, saving model to ./checkpoint-05-0.0138.hdf5\n",
      "Epoch 6/24\n",
      "100/100 [==============================] - 49s 495ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - mean_absolute_error: 0.0124 - mean_absolute_percentage_error: 47.9670 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0405 - val_mean_absolute_percentage_error: 70.8594\n",
      "\n",
      "Epoch 00006: val_mean_squared_error improved from 0.01381 to 0.01231, saving model to ./checkpoint-06-0.0123.hdf5\n",
      "Epoch 7/24\n",
      "100/100 [==============================] - 72s 717ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - mean_absolute_error: 0.0115 - mean_absolute_percentage_error: 47.2717 - val_loss: 0.0125 - val_mean_squared_error: 0.0125 - val_mean_absolute_error: 0.0396 - val_mean_absolute_percentage_error: 69.1992\n",
      "\n",
      "Epoch 00007: val_mean_squared_error did not improve\n",
      "Epoch 8/24\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 7.9355e-04 - mean_squared_error: 7.9355e-04 - mean_absolute_error: 0.0106 - mean_absolute_percentage_error: 48.7988 - val_loss: 0.0106 - val_mean_squared_error: 0.0106 - val_mean_absolute_error: 0.0367 - val_mean_absolute_percentage_error: 67.4707\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 0.01231 to 0.01055, saving model to ./checkpoint-08-0.0106.hdf5\n",
      "Epoch 9/24\n",
      "100/100 [==============================] - 55s 551ms/step - loss: 7.6536e-04 - mean_squared_error: 7.6536e-04 - mean_absolute_error: 0.0106 - mean_absolute_percentage_error: 50.1088 - val_loss: 0.0128 - val_mean_squared_error: 0.0128 - val_mean_absolute_error: 0.0395 - val_mean_absolute_percentage_error: 66.3672\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve\n",
      "Epoch 10/24\n",
      "100/100 [==============================] - 48s 485ms/step - loss: 4.3827e-04 - mean_squared_error: 4.3827e-04 - mean_absolute_error: 0.0083 - mean_absolute_percentage_error: 43.4320 - val_loss: 0.0099 - val_mean_squared_error: 0.0099 - val_mean_absolute_error: 0.0344 - val_mean_absolute_percentage_error: 65.2637\n",
      "\n",
      "Epoch 00010: val_mean_squared_error improved from 0.01055 to 0.00992, saving model to ./checkpoint-10-0.0099.hdf5\n",
      "Epoch 11/24\n",
      "100/100 [==============================] - 63s 626ms/step - loss: 3.4433e-04 - mean_squared_error: 3.4433e-04 - mean_absolute_error: 0.0071 - mean_absolute_percentage_error: 39.1743 - val_loss: 0.0093 - val_mean_squared_error: 0.0093 - val_mean_absolute_error: 0.0322 - val_mean_absolute_percentage_error: 62.2461\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 0.00992 to 0.00925, saving model to ./checkpoint-11-0.0093.hdf5\n",
      "Epoch 12/24\n",
      "100/100 [==============================] - 65s 653ms/step - loss: 0.0158 - mean_squared_error: 0.0158 - mean_absolute_error: 0.0453 - mean_absolute_percentage_error: 69.3729 - val_loss: 0.0078 - val_mean_squared_error: 0.0078 - val_mean_absolute_error: 0.0318 - val_mean_absolute_percentage_error: 65.3320\n",
      "\n",
      "Epoch 00012: val_mean_squared_error improved from 0.00925 to 0.00784, saving model to ./checkpoint-12-0.0078.hdf5\n",
      "Epoch 13/24\n",
      "100/100 [==============================] - 56s 556ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - mean_absolute_error: 0.0306 - mean_absolute_percentage_error: 64.4835 - val_loss: 0.0082 - val_mean_squared_error: 0.0082 - val_mean_absolute_error: 0.0327 - val_mean_absolute_percentage_error: 66.8164\n",
      "\n",
      "Epoch 00013: val_mean_squared_error did not improve\n",
      "Epoch 14/24\n",
      "100/100 [==============================] - 75s 748ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - mean_absolute_error: 0.0233 - mean_absolute_percentage_error: 60.7237 - val_loss: 0.0072 - val_mean_squared_error: 0.0072 - val_mean_absolute_error: 0.0298 - val_mean_absolute_percentage_error: 64.2871\n",
      "\n",
      "Epoch 00014: val_mean_squared_error improved from 0.00784 to 0.00718, saving model to ./checkpoint-14-0.0072.hdf5\n",
      "Epoch 15/24\n",
      "100/100 [==============================] - 79s 785ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - mean_absolute_error: 0.0195 - mean_absolute_percentage_error: 59.3607 - val_loss: 0.0069 - val_mean_squared_error: 0.0069 - val_mean_absolute_error: 0.0291 - val_mean_absolute_percentage_error: 63.4570\n",
      "\n",
      "Epoch 00015: val_mean_squared_error improved from 0.00718 to 0.00685, saving model to ./checkpoint-15-0.0069.hdf5\n",
      "Epoch 16/24\n",
      "100/100 [==============================] - 72s 721ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - mean_absolute_error: 0.0162 - mean_absolute_percentage_error: 53.3409 - val_loss: 0.0061 - val_mean_squared_error: 0.0061 - val_mean_absolute_error: 0.0273 - val_mean_absolute_percentage_error: 64.0527\n",
      "\n",
      "Epoch 00016: val_mean_squared_error improved from 0.00685 to 0.00610, saving model to ./checkpoint-16-0.0061.hdf5\n",
      "Epoch 17/24\n",
      "100/100 [==============================] - 91s 906ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - mean_absolute_error: 0.0126 - mean_absolute_percentage_error: 48.1615 - val_loss: 0.0054 - val_mean_squared_error: 0.0054 - val_mean_absolute_error: 0.0267 - val_mean_absolute_percentage_error: 64.3945\n",
      "\n",
      "Epoch 00017: val_mean_squared_error improved from 0.00610 to 0.00537, saving model to ./checkpoint-17-0.0054.hdf5\n",
      "Epoch 18/24\n",
      "100/100 [==============================] - 92s 920ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - mean_absolute_error: 0.0115 - mean_absolute_percentage_error: 47.2972 - val_loss: 0.0046 - val_mean_squared_error: 0.0046 - val_mean_absolute_error: 0.0252 - val_mean_absolute_percentage_error: 63.9160\n",
      "\n",
      "Epoch 00018: val_mean_squared_error improved from 0.00537 to 0.00463, saving model to ./checkpoint-18-0.0046.hdf5\n",
      "Epoch 19/24\n",
      "100/100 [==============================] - 72s 723ms/step - loss: 7.9526e-04 - mean_squared_error: 7.9526e-04 - mean_absolute_error: 0.0105 - mean_absolute_percentage_error: 48.3793 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0258 - val_mean_absolute_percentage_error: 64.3359\n",
      "\n",
      "Epoch 00019: val_mean_squared_error did not improve\n",
      "Epoch 20/24\n",
      "100/100 [==============================] - 99s 993ms/step - loss: 7.9187e-04 - mean_squared_error: 7.9187e-04 - mean_absolute_error: 0.0108 - mean_absolute_percentage_error: 50.6163 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0247 - val_mean_absolute_percentage_error: 62.0215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_mean_squared_error did not improve\n",
      "Epoch 21/24\n",
      "100/100 [==============================] - 101s 1s/step - loss: 4.4854e-04 - mean_squared_error: 4.4854e-04 - mean_absolute_error: 0.0084 - mean_absolute_percentage_error: 43.7394 - val_loss: 0.0059 - val_mean_squared_error: 0.0059 - val_mean_absolute_error: 0.0249 - val_mean_absolute_percentage_error: 60.1465\n",
      "\n",
      "Epoch 00021: val_mean_squared_error did not improve\n",
      "Epoch 22/24\n",
      "100/100 [==============================] - 96s 962ms/step - loss: 3.4717e-04 - mean_squared_error: 3.4717e-04 - mean_absolute_error: 0.0071 - mean_absolute_percentage_error: 39.0628 - val_loss: 0.0055 - val_mean_squared_error: 0.0055 - val_mean_absolute_error: 0.0242 - val_mean_absolute_percentage_error: 60.4980\n",
      "\n",
      "Epoch 00022: val_mean_squared_error did not improve\n",
      "Epoch 23/24\n",
      "100/100 [==============================] - 70s 701ms/step - loss: 0.0150 - mean_squared_error: 0.0150 - mean_absolute_error: 0.0434 - mean_absolute_percentage_error: 67.9237 - val_loss: 0.0042 - val_mean_squared_error: 0.0042 - val_mean_absolute_error: 0.0227 - val_mean_absolute_percentage_error: 60.9766\n",
      "\n",
      "Epoch 00023: val_mean_squared_error improved from 0.00463 to 0.00424, saving model to ./checkpoint-23-0.0042.hdf5\n",
      "Epoch 24/24\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - mean_absolute_error: 0.0314 - mean_absolute_percentage_error: 64.6183 - val_loss: 0.0048 - val_mean_squared_error: 0.0048 - val_mean_absolute_error: 0.0231 - val_mean_absolute_percentage_error: 60.3711\n",
      "\n",
      "Epoch 00024: val_mean_squared_error did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0423691cf8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import horovod.keras as hvd\n",
    "\n",
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "batch_size = 1024\n",
    "num_classes = 10\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Enough epochs to demonstrate learning rate warmup and the reduction of\n",
    "# learning rate when training plateaues.\n",
    "epochs = 24\n",
    "\n",
    "def dataGenerator(validation = False):\n",
    "    while True:\n",
    "        questionOriginalFile = pd.read_csv(\"../Questions.csv\",encoding=\"utf-8\", iterator = True, chunksize=batch_size)\n",
    "        current_chunk_index = 0\n",
    "        for chunk in questionOriginalFile:\n",
    "    #         print(chunk[\"Body\"])\n",
    "            current_chunk_index += 1\n",
    "            if current_chunk_index % 100 < int(100 * validation_ratio) and not validation:\n",
    "                continue\n",
    "            if current_chunk_index % 100 >= int(100 * validation_ratio) and validation:\n",
    "                continue\n",
    "            chunk[\"Body\"] = chunk[\"Body\"].map(lambda x: one_hot(x, 10000))\n",
    "#             chunk[\"Score\"] = chunk[\"Score\"].map(lambda x: 1 if x > 0 else 0)\n",
    "            chunk[\"Score\"] = chunk[\"Score\"].map(lambda x: 1 if x > 100 else x / 100)\n",
    "    #         print(chunk[\"Score\"])\n",
    "            body = pd.DataFrame(item for item in chunk[\"Body\"])\n",
    "            body = body.as_matrix()\n",
    "            body = np.nan_to_num(body)\n",
    "    #         chunk[\"Body\"].astype(\"int32\").as_matrix()\n",
    "            score = chunk[\"Score\"].as_matrix()\n",
    "            body = pad_sequences(body, maxlen=1000, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "            yield body, score\n",
    "            \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 100, input_length = 1000))\n",
    "model.add(Conv1D(32, kernel_size=3,\n",
    "                 activation='relu'))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Horovod: add Horovod Distributed Optimizer.\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "model.compile(loss=keras.losses.mse,\n",
    "              optimizer=opt,\n",
    "              metrics=['mse', 'mae', 'mape'])\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "\n",
    "    # Horovod: average metrics among workers at the end of every epoch.\n",
    "    #\n",
    "    # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "    # TensorBoard or other metrics-based callbacks.\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "\n",
    "    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
    "    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
    "    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "#     hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n",
    "\n",
    "    # Reduce the learning rate if training plateaues.\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=10, verbose=1),\n",
    "]\n",
    "\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch:02d}-{val_mean_squared_error:.4f}.hdf5', \\\n",
    "                                                    monitor='val_mean_squared_error', verbose=1, save_best_only=True, mode='min'))\n",
    "\n",
    "# print(next(dataGenerator()))\n",
    "# Train the model.\n",
    "# Horovod: the training will randomly sample 1 / N batches of training data and\n",
    "# 3 / N batches of validation data on every worker, where N is the number of workers.\n",
    "# Over-sampling of validation data helps to increase probability that every validation\n",
    "# example will be evaluated.\n",
    "model.fit_generator(dataGenerator(validation = False),\n",
    "                    steps_per_epoch = 100 // hvd.size(),\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=dataGenerator(validation = True),\n",
    "                    validation_steps = 10 // hvd.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intend to predict answer upvotes(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "100/100 [==============================] - 345s 3s/step - loss: 0.0282 - mean_squared_error: 0.0282 - mean_absolute_error: 0.0791 - mean_absolute_percentage_error: 13993677.3475 - val_loss: 0.0284 - val_mean_squared_error: 0.0284 - val_mean_absolute_error: 0.0658 - val_mean_absolute_percentage_error: 5595068.6000\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.02835, saving model to ./checkpoint-01-0.0284.hdf5\n",
      "Epoch 2/24\n",
      "100/100 [==============================] - 309s 3s/step - loss: 0.0068 - mean_squared_error: 0.0068 - mean_absolute_error: 0.0326 - mean_absolute_percentage_error: 7466312.9800 - val_loss: 0.0159 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0442 - val_mean_absolute_percentage_error: 4025208.4000\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.02835 to 0.01592, saving model to ./checkpoint-02-0.0159.hdf5\n",
      "Epoch 3/24\n",
      "100/100 [==============================] - 306s 3s/step - loss: 0.0045 - mean_squared_error: 0.0045 - mean_absolute_error: 0.0252 - mean_absolute_percentage_error: 5982377.3900 - val_loss: 0.0144 - val_mean_squared_error: 0.0144 - val_mean_absolute_error: 0.0426 - val_mean_absolute_percentage_error: 3156089.5750\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.01592 to 0.01441, saving model to ./checkpoint-03-0.0144.hdf5\n",
      "Epoch 4/24\n",
      "100/100 [==============================] - 311s 3s/step - loss: 0.0029 - mean_squared_error: 0.0029 - mean_absolute_error: 0.0207 - mean_absolute_percentage_error: 4818749.5475 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0375 - val_mean_absolute_percentage_error: 1945788.9000\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.01441 to 0.01158, saving model to ./checkpoint-04-0.0116.hdf5\n",
      "Epoch 5/24\n",
      "100/100 [==============================] - 310s 3s/step - loss: 0.0024 - mean_squared_error: 0.0024 - mean_absolute_error: 0.0178 - mean_absolute_percentage_error: 4214259.1825 - val_loss: 0.0102 - val_mean_squared_error: 0.0102 - val_mean_absolute_error: 0.0351 - val_mean_absolute_percentage_error: 1663575.3000\n",
      "\n",
      "Epoch 5: finished gradual learning rate warmup to 0.001.\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.01158 to 0.01021, saving model to ./checkpoint-05-0.0102.hdf5\n",
      "Epoch 6/24\n",
      "100/100 [==============================] - 306s 3s/step - loss: 0.0015 - mean_squared_error: 0.0015 - mean_absolute_error: 0.0139 - mean_absolute_percentage_error: 3255105.5175 - val_loss: 0.0113 - val_mean_squared_error: 0.0113 - val_mean_absolute_error: 0.0370 - val_mean_absolute_percentage_error: 990625.8438\n",
      "\n",
      "Epoch 00006: val_mean_squared_error did not improve\n",
      "Epoch 7/24\n",
      "100/100 [==============================] - 307s 3s/step - loss: 0.0013 - mean_squared_error: 0.0013 - mean_absolute_error: 0.0127 - mean_absolute_percentage_error: 2363376.2212 - val_loss: 0.0085 - val_mean_squared_error: 0.0085 - val_mean_absolute_error: 0.0306 - val_mean_absolute_percentage_error: 973613.1312\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.01021 to 0.00851, saving model to ./checkpoint-07-0.0085.hdf5\n",
      "Epoch 8/24\n",
      "100/100 [==============================] - 310s 3s/step - loss: 7.7158e-04 - mean_squared_error: 7.7158e-04 - mean_absolute_error: 0.0115 - mean_absolute_percentage_error: 1825977.7363 - val_loss: 0.0081 - val_mean_squared_error: 0.0081 - val_mean_absolute_error: 0.0318 - val_mean_absolute_percentage_error: 1253348.6000\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 0.00851 to 0.00814, saving model to ./checkpoint-08-0.0081.hdf5\n",
      "Epoch 9/24\n",
      "100/100 [==============================] - 304s 3s/step - loss: 7.4141e-04 - mean_squared_error: 7.4141e-04 - mean_absolute_error: 0.0115 - mean_absolute_percentage_error: 2066523.2463 - val_loss: 0.0070 - val_mean_squared_error: 0.0070 - val_mean_absolute_error: 0.0289 - val_mean_absolute_percentage_error: 586735.5000\n",
      "\n",
      "Epoch 00009: val_mean_squared_error improved from 0.00814 to 0.00701, saving model to ./checkpoint-09-0.0070.hdf5\n",
      "Epoch 10/24\n",
      "100/100 [==============================] - 301s 3s/step - loss: 4.2843e-04 - mean_squared_error: 4.2843e-04 - mean_absolute_error: 0.0091 - mean_absolute_percentage_error: 1434710.1175 - val_loss: 0.0065 - val_mean_squared_error: 0.0065 - val_mean_absolute_error: 0.0278 - val_mean_absolute_percentage_error: 378880.3000\n",
      "\n",
      "Epoch 00010: val_mean_squared_error improved from 0.00701 to 0.00649, saving model to ./checkpoint-10-0.0065.hdf5\n",
      "Epoch 11/24\n",
      "100/100 [==============================] - 302s 3s/step - loss: 3.4149e-04 - mean_squared_error: 3.4149e-04 - mean_absolute_error: 0.0078 - mean_absolute_percentage_error: 995061.3081 - val_loss: 0.0053 - val_mean_squared_error: 0.0053 - val_mean_absolute_error: 0.0266 - val_mean_absolute_percentage_error: 197404.0156\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 0.00649 to 0.00534, saving model to ./checkpoint-11-0.0053.hdf5\n",
      "Epoch 12/24\n",
      "100/100 [==============================] - 300s 3s/step - loss: 0.0145 - mean_squared_error: 0.0145 - mean_absolute_error: 0.0497 - mean_absolute_percentage_error: 9020126.1217 - val_loss: 0.0043 - val_mean_squared_error: 0.0043 - val_mean_absolute_error: 0.0324 - val_mean_absolute_percentage_error: 11331001.6000\n",
      "\n",
      "Epoch 00012: val_mean_squared_error improved from 0.00534 to 0.00433, saving model to ./checkpoint-12-0.0043.hdf5\n",
      "Epoch 13/24\n",
      "100/100 [==============================] - 303s 3s/step - loss: 0.0071 - mean_squared_error: 0.0071 - mean_absolute_error: 0.0339 - mean_absolute_percentage_error: 8353870.3400 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0242 - val_mean_absolute_percentage_error: 5756175.9500\n",
      "\n",
      "Epoch 00013: val_mean_squared_error did not improve\n",
      "Epoch 14/24\n",
      "100/100 [==============================] - 305s 3s/step - loss: 0.0044 - mean_squared_error: 0.0044 - mean_absolute_error: 0.0254 - mean_absolute_percentage_error: 6236075.7750 - val_loss: 0.0050 - val_mean_squared_error: 0.0050 - val_mean_absolute_error: 0.0231 - val_mean_absolute_percentage_error: 4133433.5750\n",
      "\n",
      "Epoch 00014: val_mean_squared_error did not improve\n",
      "Epoch 15/24\n",
      "100/100 [==============================] - 309s 3s/step - loss: 0.0030 - mean_squared_error: 0.0030 - mean_absolute_error: 0.0210 - mean_absolute_percentage_error: 4980282.1975 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0220 - val_mean_absolute_percentage_error: 3068862.4000\n",
      "\n",
      "Epoch 00015: val_mean_squared_error did not improve\n",
      "Epoch 16/24\n",
      "100/100 [==============================] - 309s 3s/step - loss: 0.0025 - mean_squared_error: 0.0025 - mean_absolute_error: 0.0180 - mean_absolute_percentage_error: 4156822.6025 - val_loss: 0.0043 - val_mean_squared_error: 0.0043 - val_mean_absolute_error: 0.0219 - val_mean_absolute_percentage_error: 2254644.0000\n",
      "\n",
      "Epoch 00016: val_mean_squared_error improved from 0.00433 to 0.00429, saving model to ./checkpoint-16-0.0043.hdf5\n",
      "Epoch 17/24\n",
      "100/100 [==============================] - 311s 3s/step - loss: 0.0015 - mean_squared_error: 0.0015 - mean_absolute_error: 0.0142 - mean_absolute_percentage_error: 3280805.6075 - val_loss: 0.0037 - val_mean_squared_error: 0.0037 - val_mean_absolute_error: 0.0208 - val_mean_absolute_percentage_error: 1353513.1750\n",
      "\n",
      "Epoch 00017: val_mean_squared_error improved from 0.00429 to 0.00369, saving model to ./checkpoint-17-0.0037.hdf5\n",
      "Epoch 18/24\n",
      "100/100 [==============================] - 307s 3s/step - loss: 0.0013 - mean_squared_error: 0.0013 - mean_absolute_error: 0.0128 - mean_absolute_percentage_error: 2375341.3750 - val_loss: 0.0040 - val_mean_squared_error: 0.0040 - val_mean_absolute_error: 0.0212 - val_mean_absolute_percentage_error: 1042655.7812\n",
      "\n",
      "Epoch 00018: val_mean_squared_error did not improve\n",
      "Epoch 19/24\n",
      "100/100 [==============================] - 310s 3s/step - loss: 7.7427e-04 - mean_squared_error: 7.7427e-04 - mean_absolute_error: 0.0114 - mean_absolute_percentage_error: 1783748.8875 - val_loss: 0.0042 - val_mean_squared_error: 0.0042 - val_mean_absolute_error: 0.0210 - val_mean_absolute_percentage_error: 1084337.6250\n",
      "\n",
      "Epoch 00019: val_mean_squared_error did not improve\n",
      "Epoch 20/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 305s 3s/step - loss: 7.6944e-04 - mean_squared_error: 7.6944e-04 - mean_absolute_error: 0.0117 - mean_absolute_percentage_error: 2022775.3462 - val_loss: 0.0034 - val_mean_squared_error: 0.0034 - val_mean_absolute_error: 0.0207 - val_mean_absolute_percentage_error: 713647.3500\n",
      "\n",
      "Epoch 00020: val_mean_squared_error improved from 0.00369 to 0.00336, saving model to ./checkpoint-20-0.0034.hdf5\n",
      "Epoch 21/24\n",
      "100/100 [==============================] - 305s 3s/step - loss: 4.3907e-04 - mean_squared_error: 4.3907e-04 - mean_absolute_error: 0.0092 - mean_absolute_percentage_error: 1479617.6838 - val_loss: 0.0030 - val_mean_squared_error: 0.0030 - val_mean_absolute_error: 0.0197 - val_mean_absolute_percentage_error: 412209.8719\n",
      "\n",
      "Epoch 00021: val_mean_squared_error improved from 0.00336 to 0.00299, saving model to ./checkpoint-21-0.0030.hdf5\n",
      "Epoch 22/24\n",
      "100/100 [==============================] - 309s 3s/step - loss: 3.4396e-04 - mean_squared_error: 3.4396e-04 - mean_absolute_error: 0.0078 - mean_absolute_percentage_error: 1027399.0506 - val_loss: 0.0032 - val_mean_squared_error: 0.0032 - val_mean_absolute_error: 0.0210 - val_mean_absolute_percentage_error: 253097.4641\n",
      "\n",
      "Epoch 00022: val_mean_squared_error did not improve\n",
      "Epoch 23/24\n",
      "100/100 [==============================] - 310s 3s/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0470 - mean_absolute_percentage_error: 7837081.6803 - val_loss: 0.0028 - val_mean_squared_error: 0.0028 - val_mean_absolute_error: 0.0250 - val_mean_absolute_percentage_error: 9996578.8000\n",
      "\n",
      "Epoch 00023: val_mean_squared_error improved from 0.00299 to 0.00285, saving model to ./checkpoint-23-0.0028.hdf5\n",
      "Epoch 24/24\n",
      "100/100 [==============================] - 307s 3s/step - loss: 0.0076 - mean_squared_error: 0.0076 - mean_absolute_error: 0.0349 - mean_absolute_percentage_error: 8553989.7350 - val_loss: 0.0030 - val_mean_squared_error: 0.0030 - val_mean_absolute_error: 0.0212 - val_mean_absolute_percentage_error: 6809850.5500\n",
      "\n",
      "Epoch 00024: val_mean_squared_error did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0422f87c18>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import horovod.keras as hvd\n",
    "\n",
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "batch_size = 1024\n",
    "num_classes = 10\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Enough epochs to demonstrate learning rate warmup and the reduction of\n",
    "# learning rate when training plateaues.\n",
    "epochs = 24\n",
    "\n",
    "def dataGenerator(validation = False):\n",
    "    while True:\n",
    "        questionOriginalFile = pd.read_csv(\"../Questions.csv\",encoding=\"utf-8\", iterator = True, chunksize=batch_size)\n",
    "        current_chunk_index = 0\n",
    "        for chunk in questionOriginalFile:\n",
    "    #         print(chunk[\"Body\"])\n",
    "            current_chunk_index += 1\n",
    "            if current_chunk_index % 100 < int(100 * validation_ratio) and not validation:\n",
    "                continue\n",
    "            if current_chunk_index % 100 >= int(100 * validation_ratio) and validation:\n",
    "                continue\n",
    "            chunk[\"Body\"] = chunk[\"Body\"].map(lambda x: one_hot(x, 10000))\n",
    "#             chunk[\"Score\"] = chunk[\"Score\"].map(lambda x: 1 if x > 0 else 0)\n",
    "            chunk[\"Score\"] = chunk[\"Score\"].map(lambda x: 1 if x > 100 else x / 100)\n",
    "    #         print(chunk[\"Score\"])\n",
    "            body = pd.DataFrame(item for item in chunk[\"Body\"])\n",
    "            body = body.as_matrix()\n",
    "            body = np.nan_to_num(body)\n",
    "    #         chunk[\"Body\"].astype(\"int32\").as_matrix()\n",
    "            score = chunk[\"Score\"].as_matrix()\n",
    "            body = pad_sequences(body, maxlen=1000, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "            yield body, score\n",
    "            \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 100, input_length = 1000))\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "# model.add(Conv1D(64, 3, activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(LSTM(100, return_sequences = False))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Horovod: add Horovod Distributed Optimizer.\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "model.compile(loss=keras.losses.mse,\n",
    "              optimizer=opt,\n",
    "              metrics=['mse', 'mae', 'mape'])\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "\n",
    "    # Horovod: average metrics among workers at the end of every epoch.\n",
    "    #\n",
    "    # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "    # TensorBoard or other metrics-based callbacks.\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "\n",
    "    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
    "    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
    "    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n",
    "\n",
    "    # Reduce the learning rate if training plateaues.\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=10, verbose=1),\n",
    "]\n",
    "\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch:02d}-{val_mean_squared_error:.4f}.hdf5', \\\n",
    "                                                    monitor='val_mean_squared_error', verbose=1, save_best_only=True, mode='min'))\n",
    "\n",
    "# print(next(dataGenerator()))\n",
    "# Train the model.\n",
    "# Horovod: the training will randomly sample 1 / N batches of training data and\n",
    "# 3 / N batches of validation data on every worker, where N is the number of workers.\n",
    "# Over-sampling of validation data helps to increase probability that every validation\n",
    "# example will be evaluated.\n",
    "model.fit_generator(dataGenerator(validation = False),\n",
    "                    steps_per_epoch = 100 // hvd.size(),\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=dataGenerator(validation = True),\n",
    "                    validation_steps = 10 // hvd.size())\n",
    "\n",
    "# # Evaluate the model on the full data set.\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.008448001376236788\n",
      "Test accuracy: 0.008448001376236788\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate_generator(dataGenerator(validation = True), 100)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.008448001376236788, 0.008448001376236788, 0.034822447281330826, 10765704.1175]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[9733, 6599, 6705, ...,    0,    0,    0],\n",
      "       [9733,  829, 3821, ...,    0,    0,    0],\n",
      "       [9733, 3410, 1751, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [9733, 5488, 1078, ...,    0,    0,    0],\n",
      "       [9733, 7434, 5551, ...,    0,    0,    0],\n",
      "       [9733, 6562, 5488, ...,    0,    0,    0]], dtype=int32), array([0.26, 1.  , 0.21, ..., 0.05, 0.01, 1.  ]))\n"
     ]
    }
   ],
   "source": [
    "predictData = next(dataGenerator(validation = True))\n",
    "print(predictData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(predictData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26, 1.  , 0.21, ..., 0.05, 0.01, 1.  ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26 [4.589336e-22]\n",
      "1.0 [3.9083568e-23]\n",
      "0.21 [5.872131e-23]\n",
      "0.53 [4.3699442e-23]\n",
      "0.49 [2.5148252e-22]\n",
      "0.29 [2.0163759e-22]\n",
      "0.13 [4.296979e-23]\n",
      "0.21 [4.737998e-22]\n",
      "0.79 [1.1739499e-22]\n",
      "0.09 [2.623257e-22]\n",
      "0.28 [3.139135e-23]\n",
      "0.14 [7.8038904e-23]\n",
      "0.42 [4.7563586e-23]\n",
      "0.17 [3.6593683e-23]\n",
      "0.36 [5.938789e-23]\n",
      "0.17 [6.0162944e-23]\n",
      "0.23 [9.624909e-23]\n",
      "0.18 [6.218364e-23]\n",
      "0.18 [3.713367e-23]\n",
      "0.63 [7.181113e-23]\n",
      "0.51 [6.0691266e-23]\n",
      "0.13 [3.7145002e-23]\n",
      "0.1 [3.1323647e-23]\n",
      "0.77 [7.191393e-23]\n",
      "0.83 [4.900046e-23]\n",
      "0.1 [2.8790504e-23]\n",
      "1.0 [3.294746e-23]\n",
      "0.39 [3.444852e-23]\n",
      "0.16 [4.3435357e-23]\n",
      "0.14 [3.7650586e-23]\n",
      "0.07 [5.8845546e-23]\n",
      "0.27 [4.3496042e-23]\n",
      "0.12 [1.04315325e-22]\n",
      "0.32 [5.1579976e-23]\n",
      "0.14 [2.6617129e-22]\n",
      "0.17 [2.515754e-16]\n",
      "0.76 [4.0114937e-23]\n",
      "0.29 [5.1434378e-23]\n",
      "0.25 [4.1778862e-23]\n",
      "0.46 [4.584139e-22]\n",
      "0.25 [3.0358197e-22]\n",
      "0.12 [1.3592468e-22]\n",
      "0.16 [2.879907e-23]\n",
      "0.41 [4.765439e-23]\n",
      "0.03 [2.2419513e-22]\n",
      "0.92 [8.967194e-23]\n",
      "0.07 [5.052602e-23]\n",
      "0.09 [4.8220536e-23]\n",
      "0.34 [9.5438166e-23]\n",
      "0.02 [5.849439e-23]\n",
      "0.46 [5.1252817e-23]\n",
      "0.11 [4.469293e-23]\n",
      "0.08 [9.821935e-23]\n",
      "0.15 [5.8773754e-23]\n",
      "0.16 [5.660157e-23]\n",
      "0.08 [8.265284e-23]\n",
      "0.11 [2.1129502e-22]\n",
      "0.22 [6.3471565e-23]\n",
      "0.11 [1.8097446e-22]\n",
      "0.9 [4.2173574e-23]\n",
      "0.04 [4.253034e-21]\n",
      "0.03 [6.217086e-22]\n",
      "0.37 [1.7256455e-22]\n",
      "0.02 [1.04981584e-22]\n",
      "0.06 [5.9393786e-23]\n",
      "0.14 [6.630002e-23]\n",
      "0.03 [5.5047976e-22]\n",
      "0.56 [1.4272743e-22]\n",
      "0.0 [5.1426142e-23]\n",
      "0.1 [1.4344911e-21]\n",
      "0.03 [1.4048945e-22]\n",
      "0.17 [2.3478336e-22]\n",
      "0.07 [4.2123412e-23]\n",
      "0.05 [5.5112246e-23]\n",
      "0.04 [6.996269e-23]\n",
      "0.09 [7.1698356e-23]\n",
      "0.48 [2.7378667e-23]\n",
      "0.31 [3.8072898e-23]\n",
      "0.0 [6.9852024e-23]\n",
      "0.06 [1.2562303e-22]\n",
      "0.33 [3.265481e-22]\n",
      "0.15 [3.844579e-23]\n",
      "0.08 [1.4397971e-22]\n",
      "0.19 [3.6432548e-22]\n",
      "0.35 [5.1094117e-23]\n",
      "0.14 [5.873386e-23]\n",
      "0.25 [5.3385683e-22]\n",
      "0.63 [9.4422397e-23]\n",
      "0.0 [5.3062836e-23]\n",
      "0.0 [2.834999e-23]\n",
      "0.0 [7.374158e-23]\n",
      "0.0 [1.0170158e-22]\n",
      "0.06 [4.4096987e-23]\n",
      "0.1 [2.5749538e-22]\n",
      "1.0 [3.4276543e-23]\n",
      "0.01 [7.0109376e-23]\n",
      "0.14 [1.024281e-22]\n",
      "0.05 [4.6178706e-23]\n",
      "0.1 [5.485657e-23]\n",
      "0.12 [3.7852923e-23]\n",
      "0.06 [4.5318712e-23]\n",
      "0.11 [5.0556862e-23]\n",
      "0.56 [2.7158143e-23]\n",
      "0.06 [9.0878594e-23]\n",
      "0.08 [4.088102e-23]\n",
      "0.08 [4.3199745e-22]\n",
      "0.11 [5.9729146e-23]\n",
      "0.03 [9.250078e-23]\n",
      "0.08 [2.2529252e-22]\n",
      "0.05 [5.1608566e-21]\n",
      "0.4 [3.82594e-23]\n",
      "0.08 [1.2901777e-22]\n",
      "0.29 [8.965791e-23]\n",
      "0.18 [2.2298313e-22]\n",
      "0.01 [9.0419035e-23]\n",
      "0.08 [1.3883099e-22]\n",
      "0.05 [2.8765807e-23]\n",
      "0.03 [5.9321324e-23]\n",
      "0.24 [1.5342003e-22]\n",
      "0.03 [3.8800278e-23]\n",
      "1.0 [4.011555e-23]\n",
      "0.06 [1.1288129e-22]\n",
      "0.06 [3.2526985e-23]\n",
      "0.15 [1.9701778e-22]\n",
      "0.04 [7.0390253e-22]\n",
      "0.04 [1.1009743e-22]\n",
      "0.09 [4.9627262e-23]\n",
      "0.22 [1.684077e-22]\n",
      "0.02 [5.475206e-22]\n",
      "0.06 [4.373079e-23]\n",
      "0.06 [4.460675e-23]\n",
      "0.02 [3.6691808e-23]\n",
      "0.09 [8.283122e-22]\n",
      "0.12 [2.9939442e-23]\n",
      "0.15 [7.187032e-23]\n",
      "0.04 [5.2171067e-23]\n",
      "0.15 [4.0900674e-23]\n",
      "0.02 [8.829286e-22]\n",
      "0.06 [8.629655e-23]\n",
      "0.06 [6.111107e-23]\n",
      "0.08 [4.6535208e-23]\n",
      "0.07 [4.756921e-23]\n",
      "0.04 [3.513883e-22]\n",
      "0.83 [1.5218895e-22]\n",
      "0.02 [1.2197564e-21]\n",
      "0.34 [4.1529238e-23]\n",
      "0.14 [8.1354045e-23]\n",
      "0.14 [2.9734682e-23]\n",
      "0.1 [3.1895423e-23]\n",
      "0.15 [2.8763282e-23]\n",
      "0.02 [1.5050084e-22]\n",
      "0.06 [2.997624e-23]\n",
      "0.23 [4.012427e-23]\n",
      "1.0 [9.332733e-23]\n",
      "0.0 [7.0888726e-23]\n",
      "0.03 [8.8264864e-23]\n",
      "0.53 [6.40634e-23]\n",
      "0.04 [7.8233835e-23]\n",
      "0.02 [6.717731e-23]\n",
      "0.32 [5.1021274e-23]\n",
      "0.03 [3.1698523e-18]\n",
      "0.26 [1.5398114e-22]\n",
      "0.15 [3.2072705e-23]\n",
      "0.08 [7.1507155e-23]\n",
      "0.02 [1.1396382e-22]\n",
      "0.16 [4.8384165e-23]\n",
      "0.0 [4.2725918e-23]\n",
      "1.0 [5.3955233e-23]\n",
      "0.04 [9.465285e-23]\n",
      "0.38 [2.531737e-22]\n",
      "0.03 [3.9481567e-23]\n",
      "0.05 [3.213774e-23]\n",
      "0.39 [3.3649126e-22]\n",
      "0.02 [6.1143485e-23]\n",
      "0.38 [6.353528e-23]\n",
      "0.28 [9.982927e-23]\n",
      "0.05 [5.959123e-23]\n",
      "0.48 [4.8738537e-23]\n",
      "0.21 [9.3657587e-23]\n",
      "0.01 [5.2665357e-23]\n",
      "0.05 [4.2446176e-23]\n",
      "0.04 [5.2623788e-23]\n",
      "0.2 [3.3607163e-23]\n",
      "1.0 [4.0713253e-23]\n",
      "0.24 [7.039317e-23]\n",
      "0.08 [1.9193696e-22]\n",
      "0.19 [3.6669562e-23]\n",
      "0.01 [3.7463478e-23]\n",
      "0.19 [2.6218167e-22]\n",
      "0.01 [2.3051114e-22]\n",
      "0.1 [6.7735195e-23]\n",
      "0.18 [4.10249e-23]\n",
      "0.11 [5.048575e-23]\n",
      "0.01 [1.0523298e-22]\n",
      "0.21 [4.0481574e-23]\n",
      "0.04 [7.21351e-23]\n",
      "0.24 [5.2681434e-23]\n",
      "0.04 [2.176234e-22]\n",
      "0.0 [5.061958e-23]\n",
      "0.5 [7.447405e-23]\n",
      "1.0 [4.470265e-23]\n",
      "0.01 [3.4607634e-23]\n",
      "0.09 [8.829618e-23]\n",
      "0.02 [4.5319403e-23]\n",
      "0.02 [1.7810491e-21]\n",
      "0.16 [3.3609084e-23]\n",
      "0.08 [4.8160608e-23]\n",
      "0.02 [6.156359e-23]\n",
      "0.04 [7.776314e-23]\n",
      "0.39 [9.321169e-23]\n",
      "0.08 [1.2547837e-22]\n",
      "0.03 [8.928724e-23]\n",
      "0.05 [3.2658403e-23]\n",
      "0.02 [6.0524377e-22]\n",
      "0.07 [8.724853e-23]\n",
      "0.71 [2.4435596e-22]\n",
      "0.01 [4.0942042e-23]\n",
      "0.2 [4.5284668e-23]\n",
      "0.09 [2.970872e-23]\n",
      "0.02 [1.1419404e-22]\n",
      "1.0 [8.028089e-23]\n",
      "0.3 [2.9900423e-22]\n",
      "0.29 [6.803307e-21]\n",
      "0.07 [1.3252753e-22]\n",
      "0.01 [1.2355517e-22]\n",
      "0.07 [1.1285594e-21]\n",
      "0.31 [1.0399349e-22]\n",
      "0.09 [4.888606e-21]\n",
      "0.04 [5.914012e-23]\n",
      "0.03 [1.1598658e-22]\n",
      "0.85 [7.976256e-23]\n",
      "0.08 [3.611457e-22]\n",
      "0.04 [6.1342127e-21]\n",
      "1.0 [5.460686e-23]\n",
      "0.04 [4.1705934e-23]\n",
      "0.05 [2.908746e-23]\n",
      "0.01 [6.431561e-23]\n",
      "0.47 [1.3493175e-22]\n",
      "0.05 [7.2828554e-23]\n",
      "0.02 [7.517392e-23]\n",
      "0.04 [7.985481e-23]\n",
      "0.21 [1.3700539e-22]\n",
      "0.64 [1.958645e-22]\n",
      "0.04 [6.623278e-23]\n",
      "0.16 [9.534864e-23]\n",
      "0.08 [3.5271094e-23]\n",
      "0.06 [3.2633267e-22]\n",
      "0.5 [3.8543482e-21]\n",
      "1.0 [1.1197256e-21]\n",
      "0.1 [6.3885004e-23]\n",
      "0.2 [3.6906583e-23]\n",
      "1.0 [6.313711e-23]\n",
      "0.45 [3.2700168e-23]\n",
      "0.12 [1.0554133e-22]\n",
      "0.08 [3.0569335e-23]\n",
      "0.17 [5.3955025e-23]\n",
      "0.03 [2.0495282e-22]\n",
      "0.04 [5.8559926e-19]\n",
      "0.01 [1.3233657e-22]\n",
      "0.13 [3.1578614e-22]\n",
      "0.05 [3.5580833e-23]\n",
      "0.19 [6.7010446e-23]\n",
      "0.04 [3.9706758e-16]\n",
      "0.41 [6.7602003e-23]\n",
      "0.07 [7.7916057e-23]\n",
      "0.34 [3.1530928e-23]\n",
      "0.0 [3.624885e-23]\n",
      "0.11 [4.1229168e-23]\n",
      "0.01 [4.7607513e-23]\n",
      "0.19 [3.5996537e-23]\n",
      "0.35 [4.0950476e-23]\n",
      "0.01 [3.2119493e-22]\n",
      "0.28 [3.9436106e-23]\n",
      "0.11 [5.1307005e-23]\n",
      "0.04 [2.6341974e-22]\n",
      "0.09 [6.550836e-23]\n",
      "0.26 [5.684955e-23]\n",
      "0.72 [3.2258968e-23]\n",
      "0.53 [1.4798113e-22]\n",
      "0.16 [8.1266576e-23]\n",
      "1.0 [5.028358e-22]\n",
      "0.09 [1.4819522e-22]\n",
      "0.01 [1.3833195e-22]\n",
      "0.04 [7.3240604e-23]\n",
      "0.05 [7.227448e-23]\n",
      "0.32 [7.154726e-23]\n",
      "0.2 [4.6588143e-23]\n",
      "0.03 [2.7199021e-21]\n",
      "0.06 [1.5037114e-22]\n",
      "0.44 [8.1566596e-23]\n",
      "0.01 [1.570302e-21]\n",
      "0.45 [4.5109837e-23]\n",
      "0.31 [3.1673182e-23]\n",
      "0.02 [3.030704e-23]\n",
      "0.01 [6.404972e-23]\n",
      "0.06 [5.363285e-23]\n",
      "0.01 [8.92726e-23]\n",
      "0.09 [3.782492e-23]\n",
      "0.01 [1.04990394e-22]\n",
      "0.01 [1.2666675e-22]\n",
      "0.05 [6.0204495e-23]\n",
      "0.04 [8.4582884e-23]\n",
      "0.03 [4.623776e-23]\n",
      "0.05 [3.659703e-23]\n",
      "0.03 [8.4514834e-23]\n",
      "0.05 [1.6424316e-22]\n",
      "0.1 [4.280602e-23]\n",
      "0.02 [5.56854e-22]\n",
      "0.7 [3.6770715e-22]\n",
      "0.05 [5.1543787e-23]\n",
      "0.0 [1.0360703e-22]\n",
      "0.09 [4.9477362e-23]\n",
      "0.05 [2.547995e-22]\n",
      "0.32 [4.7128494e-23]\n",
      "0.04 [7.282605e-23]\n",
      "0.09 [4.0018803e-23]\n",
      "0.02 [5.39995e-23]\n",
      "0.58 [3.6141975e-23]\n",
      "0.08 [6.1231945e-23]\n",
      "0.06 [4.269692e-23]\n",
      "0.11 [2.5057378e-22]\n",
      "1.0 [4.6980766e-23]\n",
      "0.0 [4.6823502e-23]\n",
      "0.17 [4.5835946e-23]\n",
      "0.01 [2.0352551e-22]\n",
      "0.02 [8.700192e-23]\n",
      "0.08 [8.1259136e-23]\n",
      "0.01 [9.433636e-23]\n",
      "0.0 [1.068865e-22]\n",
      "0.08 [2.677302e-23]\n",
      "0.28 [4.4191287e-23]\n",
      "0.71 [7.0024646e-23]\n",
      "0.05 [1.2207472e-22]\n",
      "0.04 [2.2966853e-22]\n",
      "0.06 [1.1559522e-22]\n",
      "0.15 [3.767659e-23]\n",
      "0.1 [3.7317425e-23]\n",
      "0.04 [7.277106e-23]\n",
      "1.0 [4.0466132e-23]\n",
      "0.53 [3.986917e-23]\n",
      "0.02 [4.845128e-20]\n",
      "0.07 [4.3962622e-23]\n",
      "0.1 [5.443486e-23]\n",
      "0.12 [7.11917e-23]\n",
      "0.89 [3.2451633e-23]\n",
      "1.0 [3.6153833e-23]\n",
      "0.13 [4.1705483e-22]\n",
      "0.08 [8.5306775e-23]\n",
      "0.06 [8.9566636e-23]\n",
      "0.16 [3.386818e-22]\n",
      "0.0 [3.4652284e-23]\n",
      "0.01 [4.7135146e-23]\n",
      "0.59 [6.1477e-23]\n",
      "0.04 [2.9544858e-22]\n",
      "0.0 [1.3145056e-22]\n",
      "0.16 [4.2914093e-23]\n",
      "0.06 [8.6550076e-23]\n",
      "0.18 [8.8981566e-23]\n",
      "0.01 [1.6074725e-22]\n",
      "0.04 [1.1800876e-22]\n",
      "0.03 [4.4946486e-23]\n",
      "0.02 [8.0972334e-22]\n",
      "1.0 [1.0872763e-22]\n",
      "0.01 [5.930597e-22]\n",
      "0.03 [4.9950515e-23]\n",
      "0.88 [3.4458773e-23]\n",
      "0.02 [6.4259694e-23]\n",
      "0.05 [8.310884e-21]\n",
      "0.23 [5.1485225e-23]\n",
      "0.04 [1.0678828e-22]\n",
      "0.17 [3.4024202e-23]\n",
      "0.29 [3.8932233e-23]\n",
      "0.01 [1.5042048e-22]\n",
      "0.04 [3.5802216e-23]\n",
      "0.48 [6.919616e-23]\n",
      "1.0 [2.926364e-23]\n",
      "1.0 [4.3615507e-23]\n",
      "0.13 [4.0941726e-23]\n",
      "0.3 [1.0639021e-22]\n",
      "0.1 [2.3878562e-22]\n",
      "0.23 [3.4501652e-23]\n",
      "0.04 [5.7136974e-23]\n",
      "0.05 [4.102036e-23]\n",
      "0.04 [8.2604295e-23]\n",
      "0.12 [4.7915424e-23]\n",
      "0.4 [9.823434e-23]\n",
      "0.01 [3.0400614e-22]\n",
      "0.01 [4.2463994e-23]\n",
      "0.01 [1.1847973e-21]\n",
      "1.0 [8.2747804e-23]\n",
      "0.02 [1.4217207e-22]\n",
      "0.37 [8.872431e-23]\n",
      "0.21 [3.693856e-23]\n",
      "0.06 [8.7798086e-23]\n",
      "0.14 [1.0551154e-22]\n",
      "0.11 [6.315662e-23]\n",
      "0.0 [3.1251798e-23]\n",
      "0.1 [3.5242986e-23]\n",
      "0.33 [2.1159186e-22]\n",
      "0.04 [1.6117953e-22]\n",
      "0.0 [3.122381e-22]\n",
      "0.1 [1.212144e-22]\n",
      "0.83 [9.9693035e-23]\n",
      "0.08 [3.6101326e-23]\n",
      "0.06 [4.549417e-23]\n",
      "0.09 [9.310864e-23]\n",
      "0.32 [3.4892104e-23]\n",
      "0.04 [3.2854088e-23]\n",
      "0.23 [6.2228725e-23]\n",
      "0.57 [7.0009146e-23]\n",
      "0.1 [1.1043225e-22]\n",
      "0.61 [2.871833e-23]\n",
      "0.02 [3.303543e-23]\n",
      "0.19 [4.4428946e-23]\n",
      "0.21 [3.0024996e-23]\n",
      "0.02 [6.486683e-22]\n",
      "0.04 [7.156555e-23]\n",
      "0.1 [5.3482075e-23]\n",
      "0.06 [7.448569e-23]\n",
      "0.65 [2.0911958e-22]\n",
      "0.1 [4.4123906e-23]\n",
      "1.0 [1.9491713e-22]\n",
      "0.0 [1.0649415e-22]\n",
      "0.05 [1.3321179e-22]\n",
      "0.03 [6.9340956e-23]\n",
      "0.27 [1.3913485e-21]\n",
      "0.03 [3.9824803e-22]\n",
      "0.22 [6.3819485e-23]\n",
      "0.14 [1.8379983e-22]\n",
      "0.01 [3.782232e-23]\n",
      "0.02 [1.4470715e-21]\n",
      "0.03 [4.881892e-23]\n",
      "1.0 [3.7905953e-23]\n",
      "0.02 [5.88909e-23]\n",
      "0.02 [7.0346457e-23]\n",
      "0.02 [3.6746716e-23]\n",
      "0.03 [8.7036115e-23]\n",
      "0.01 [3.5380516e-23]\n",
      "0.17 [5.761828e-23]\n",
      "0.09 [7.152462e-23]\n",
      "0.05 [1.1563624e-22]\n",
      "1.0 [3.8376045e-23]\n",
      "0.02 [1.3370103e-22]\n",
      "0.98 [3.6018105e-23]\n",
      "1.0 [5.483062e-23]\n",
      "0.0 [6.347423e-23]\n",
      "0.19 [6.6709516e-22]\n",
      "0.06 [6.7719436e-23]\n",
      "0.08 [5.743439e-23]\n",
      "0.06 [2.4479444e-22]\n",
      "0.86 [3.3919492e-23]\n",
      "0.04 [8.820393e-23]\n",
      "0.02 [4.2143502e-23]\n",
      "0.37 [1.7842578e-22]\n",
      "1.0 [3.2627278e-23]\n",
      "0.06 [8.3488196e-22]\n",
      "1.0 [6.721064e-23]\n",
      "0.01 [5.659876e-23]\n",
      "0.01 [6.4209706e-23]\n",
      "0.14 [1.6959132e-22]\n",
      "0.06 [1.4126492e-22]\n",
      "0.53 [7.746055e-23]\n",
      "0.07 [1.0510299e-22]\n",
      "0.11 [1.4253535e-22]\n",
      "0.01 [1.7798063e-21]\n",
      "0.03 [5.335919e-23]\n",
      "0.02 [5.556717e-23]\n",
      "0.27 [1.1802722e-22]\n",
      "0.05 [4.309405e-23]\n",
      "0.03 [1.02534815e-22]\n",
      "0.22 [3.5282132e-23]\n",
      "0.1 [3.8265096e-23]\n",
      "0.05 [6.5876234e-23]\n",
      "1.0 [3.7768398e-23]\n",
      "0.35 [4.4317732e-23]\n",
      "0.15 [7.340815e-23]\n",
      "1.0 [1.8512267e-15]\n",
      "0.22 [5.071081e-23]\n",
      "1.0 [1.4924193e-22]\n",
      "0.08 [2.408899e-22]\n",
      "0.02 [6.3405023e-23]\n",
      "1.0 [4.535607e-23]\n",
      "1.0 [1.0637722e-22]\n",
      "1.0 [8.1390673e-23]\n",
      "0.52 [8.214713e-22]\n",
      "0.0 [5.531592e-23]\n",
      "0.12 [3.118689e-23]\n",
      "-0.01 [7.0177066e-23]\n",
      "0.06 [5.92736e-23]\n",
      "0.02 [1.1545243e-22]\n",
      "0.03 [2.3389658e-22]\n",
      "0.04 [2.9763735e-23]\n",
      "0.11 [5.8703393e-23]\n",
      "0.02 [4.7442174e-23]\n",
      "0.02 [3.2478262e-23]\n",
      "0.02 [4.227505e-23]\n",
      "0.28 [4.1953747e-23]\n",
      "0.07 [5.4839824e-23]\n",
      "0.07 [8.01221e-23]\n",
      "0.06 [4.742643e-23]\n",
      "0.02 [5.635059e-23]\n",
      "0.09 [3.5553696e-23]\n",
      "0.81 [1.3501104e-22]\n",
      "1.0 [1.0757944e-22]\n",
      "0.05 [6.046942e-23]\n",
      "0.05 [4.6187866e-23]\n",
      "0.22 [5.302884e-23]\n",
      "0.49 [5.054935e-23]\n",
      "0.15 [3.5888088e-23]\n",
      "0.07 [3.1003865e-23]\n",
      "0.08 [9.530065e-23]\n",
      "0.05 [5.0349584e-23]\n",
      "0.06 [3.5232709e-19]\n",
      "0.09 [9.411852e-23]\n",
      "0.06 [6.766366e-23]\n",
      "0.2 [3.4854244e-20]\n",
      "1.0 [4.3873828e-23]\n",
      "0.04 [4.6249577e-23]\n",
      "0.05 [7.5619146e-23]\n",
      "0.18 [9.815456e-23]\n",
      "0.01 [6.7676566e-23]\n",
      "0.06 [1.02647925e-22]\n",
      "0.47 [2.657695e-22]\n",
      "0.08 [4.371962e-23]\n",
      "0.07 [6.022402e-23]\n",
      "0.07 [1.5647343e-22]\n",
      "0.04 [4.0738734e-23]\n",
      "0.32 [6.165478e-23]\n",
      "0.03 [4.3261394e-23]\n",
      "0.0 [4.9947848e-23]\n",
      "0.06 [5.1535138e-23]\n",
      "0.22 [5.0231595e-23]\n",
      "0.1 [8.597159e-23]\n",
      "0.0 [9.3741225e-23]\n",
      "0.02 [4.6896456e-22]\n",
      "0.07 [3.1432453e-23]\n",
      "0.28 [9.2690816e-23]\n",
      "0.01 [1.0936865e-22]\n",
      "0.04 [1.0721073e-22]\n",
      "0.06 [1.158685e-22]\n",
      "0.19 [6.3121217e-23]\n",
      "0.13 [1.2835262e-22]\n",
      "0.18 [8.294627e-23]\n",
      "0.06 [4.2210754e-23]\n",
      "0.56 [3.152203e-23]\n",
      "0.02 [1.1526937e-22]\n",
      "0.01 [1.3351703e-22]\n",
      "0.04 [9.3148065e-23]\n",
      "0.04 [1.6174925e-22]\n",
      "0.01 [6.066978e-22]\n",
      "0.7 [4.7388815e-23]\n",
      "0.24 [6.070632e-23]\n",
      "0.3 [3.5625653e-23]\n",
      "0.16 [7.119931e-23]\n",
      "0.04 [1.3163924e-22]\n",
      "0.08 [2.052611e-22]\n",
      "0.07 [4.6538758e-23]\n",
      "0.01 [1.871549e-22]\n",
      "0.18 [8.1220406e-23]\n",
      "0.06 [3.0830256e-23]\n",
      "0.02 [2.901587e-23]\n",
      "0.02 [4.2192402e-23]\n",
      "0.02 [1.9190549e-22]\n",
      "0.02 [1.3065318e-22]\n",
      "0.02 [3.4424744e-23]\n",
      "0.04 [1.9011521e-22]\n",
      "0.14 [3.4996615e-23]\n",
      "0.02 [5.939718e-23]\n",
      "0.0 [1.2178449e-22]\n",
      "0.11 [5.1121995e-23]\n",
      "0.03 [5.0031947e-23]\n",
      "0.07 [1.0826737e-22]\n",
      "0.09 [4.180246e-23]\n",
      "0.28 [4.7445433e-23]\n",
      "0.0 [5.3447194e-23]\n",
      "0.19 [1.4068948e-22]\n",
      "0.05 [2.539272e-22]\n",
      "0.3 [4.2515214e-23]\n",
      "0.02 [1.392028e-21]\n",
      "0.16 [1.0121199e-22]\n",
      "0.08 [1.7709469e-22]\n",
      "0.51 [5.4168264e-23]\n",
      "0.01 [4.5644363e-23]\n",
      "1.0 [3.5825307e-23]\n",
      "0.06 [5.1497256e-21]\n",
      "1.0 [3.931924e-23]\n",
      "0.0 [4.5129654e-22]\n",
      "0.08 [4.415085e-23]\n",
      "0.05 [4.0799072e-23]\n",
      "0.05 [1.881492e-22]\n",
      "0.05 [6.6029985e-22]\n",
      "0.0 [1.3826864e-22]\n",
      "0.07 [3.6660332e-23]\n",
      "0.67 [3.3150942e-23]\n",
      "0.03 [2.7154725e-23]\n",
      "0.06 [1.7704808e-22]\n",
      "0.06 [6.083684e-23]\n",
      "0.58 [7.275635e-23]\n",
      "0.13 [3.0169083e-23]\n",
      "0.03 [2.357859e-22]\n",
      "0.0 [1.7065042e-22]\n",
      "0.01 [7.326408e-23]\n",
      "0.02 [8.099855e-23]\n",
      "0.75 [2.7175867e-23]\n",
      "0.07 [5.3394827e-23]\n",
      "0.01 [6.0340154e-23]\n",
      "0.01 [1.0941289e-22]\n",
      "0.26 [3.279023e-23]\n",
      "0.54 [9.1831714e-23]\n",
      "0.08 [2.7756506e-23]\n",
      "0.04 [4.5638617e-23]\n",
      "0.04 [2.749475e-22]\n",
      "0.02 [5.1447135e-23]\n",
      "0.02 [6.2869854e-23]\n",
      "0.03 [3.7185407e-23]\n",
      "0.21 [1.3457843e-17]\n",
      "0.01 [6.12532e-23]\n",
      "0.03 [3.1596929e-22]\n",
      "0.03 [1.9961127e-20]\n",
      "0.13 [4.4414712e-23]\n",
      "0.08 [1.1587998e-22]\n",
      "0.13 [1.1915547e-22]\n",
      "0.19 [1.769522e-22]\n",
      "0.14 [2.9191855e-22]\n",
      "0.0 [5.554471e-23]\n",
      "0.52 [6.558537e-23]\n",
      "0.05 [1.0503532e-21]\n",
      "0.01 [5.2712793e-23]\n",
      "0.51 [3.017357e-23]\n",
      "0.04 [7.8090757e-22]\n",
      "0.03 [3.631334e-23]\n",
      "0.06 [6.1636675e-23]\n",
      "0.0 [1.9689978e-22]\n",
      "0.04 [4.0860284e-23]\n",
      "0.11 [7.350566e-23]\n",
      "0.43 [1.6522538e-22]\n",
      "0.04 [1.9935763e-22]\n",
      "0.02 [3.3443865e-22]\n",
      "0.01 [7.6049755e-19]\n",
      "0.04 [4.409261e-23]\n",
      "0.03 [2.9255728e-22]\n",
      "0.02 [5.3939387e-23]\n",
      "0.0 [6.118174e-23]\n",
      "0.49 [5.5357506e-23]\n",
      "0.07 [4.309619e-23]\n",
      "0.24 [7.404884e-23]\n",
      "0.06 [2.0074194e-22]\n",
      "0.05 [1.9250462e-21]\n",
      "0.15 [6.5588376e-23]\n",
      "0.0 [8.4916844e-23]\n",
      "0.0 [7.3798424e-23]\n",
      "0.05 [1.5478384e-22]\n",
      "0.02 [1.1618718e-22]\n",
      "0.06 [5.5751476e-23]\n",
      "0.01 [8.969417e-23]\n",
      "0.33 [3.036444e-23]\n",
      "0.71 [1.2787954e-22]\n",
      "1.0 [3.2230307e-23]\n",
      "0.08 [6.8405127e-23]\n",
      "0.06 [2.771124e-22]\n",
      "0.29 [1.7166571e-22]\n",
      "0.1 [4.9570525e-22]\n",
      "0.03 [8.410156e-23]\n",
      "0.23 [3.148766e-23]\n",
      "0.02 [4.807214e-23]\n",
      "0.15 [2.2649201e-22]\n",
      "0.02 [2.996664e-23]\n",
      "0.23 [4.9302895e-23]\n",
      "0.04 [3.1768856e-18]\n",
      "0.01 [8.291147e-23]\n",
      "0.03 [3.2625905e-23]\n",
      "0.16 [6.260134e-23]\n",
      "0.02 [3.568441e-23]\n",
      "0.03 [6.120602e-23]\n",
      "0.07 [5.4705295e-22]\n",
      "0.01 [1.0833679e-22]\n",
      "0.14 [1.2400705e-22]\n",
      "0.07 [3.1890803e-23]\n",
      "0.24 [2.6565597e-22]\n",
      "0.0 [3.4170012e-23]\n",
      "0.46 [7.499147e-23]\n",
      "0.71 [5.638197e-23]\n",
      "1.0 [4.0324828e-23]\n",
      "0.03 [2.5576943e-22]\n",
      "0.06 [8.4542236e-23]\n",
      "0.07 [5.3652904e-23]\n",
      "0.06 [5.584555e-23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08 [4.869765e-23]\n",
      "0.69 [4.2752983e-23]\n",
      "0.11 [3.3180676e-23]\n",
      "0.75 [3.271277e-23]\n",
      "0.33 [1.4544646e-22]\n",
      "0.03 [3.5127288e-23]\n",
      "0.0 [1.127139e-22]\n",
      "0.02 [1.164321e-22]\n",
      "0.13 [3.8308177e-23]\n",
      "0.0 [3.71925e-23]\n",
      "0.05 [8.788323e-22]\n",
      "0.01 [6.9960293e-23]\n",
      "0.02 [6.366144e-23]\n",
      "0.04 [6.99667e-23]\n",
      "0.05 [6.800912e-23]\n",
      "0.15 [3.00627e-23]\n",
      "0.42 [1.1047816e-22]\n",
      "1.0 [1.0685714e-22]\n",
      "0.02 [3.104564e-23]\n",
      "0.04 [7.412175e-23]\n",
      "0.02 [4.1463567e-22]\n",
      "0.06 [3.253853e-23]\n",
      "0.78 [4.246901e-23]\n",
      "0.37 [2.2101505e-22]\n",
      "0.09 [3.580577e-23]\n",
      "0.02 [4.5078358e-23]\n",
      "0.04 [7.13415e-23]\n",
      "1.0 [3.6005193e-23]\n",
      "0.05 [9.499213e-23]\n",
      "0.03 [3.6244284e-23]\n",
      "0.05 [2.9579917e-23]\n",
      "0.05 [6.2866496e-23]\n",
      "0.05 [6.773262e-23]\n",
      "0.91 [2.6190376e-22]\n",
      "0.1 [3.7485514e-22]\n",
      "0.06 [2.9574613e-23]\n",
      "0.08 [4.5878102e-23]\n",
      "0.14 [4.6432006e-23]\n",
      "0.04 [3.798093e-23]\n",
      "0.1 [3.1469886e-23]\n",
      "0.0 [4.6580498e-23]\n",
      "0.12 [1.079593e-22]\n",
      "0.33 [2.1243147e-21]\n",
      "0.09 [5.6695137e-23]\n",
      "0.02 [5.176309e-23]\n",
      "1.0 [1.9355456e-22]\n",
      "0.06 [1.7624888e-22]\n",
      "0.1 [3.3096244e-22]\n",
      "0.31 [7.2062527e-22]\n",
      "0.0 [8.748249e-23]\n",
      "0.07 [4.9943465e-23]\n",
      "0.53 [7.423154e-23]\n",
      "1.0 [4.184825e-23]\n",
      "0.97 [4.9088394e-23]\n",
      "0.78 [5.1343028e-23]\n",
      "0.1 [5.691226e-23]\n",
      "0.07 [6.589533e-23]\n",
      "0.07 [6.777112e-23]\n",
      "0.01 [6.626235e-23]\n",
      "0.03 [6.14838e-23]\n",
      "0.56 [9.0395237e-23]\n",
      "0.25 [6.596097e-23]\n",
      "0.0 [2.995715e-23]\n",
      "0.05 [6.335594e-23]\n",
      "0.1 [3.0581932e-23]\n",
      "0.06 [1.1952883e-21]\n",
      "0.01 [5.2873705e-23]\n",
      "1.0 [5.873923e-23]\n",
      "0.12 [3.4363343e-23]\n",
      "0.03 [5.810032e-23]\n",
      "0.02 [1.04584673e-22]\n",
      "0.0 [3.868131e-23]\n",
      "0.01 [3.4342375e-23]\n",
      "0.0 [4.2210915e-23]\n",
      "1.0 [4.506976e-23]\n",
      "0.09 [3.7285282e-22]\n",
      "0.01 [5.39577e-23]\n",
      "1.0 [1.4981519e-22]\n",
      "0.11 [4.658885e-23]\n",
      "0.01 [1.0646368e-22]\n",
      "0.04 [4.1812013e-16]\n",
      "0.0 [4.4925742e-23]\n",
      "0.05 [4.5809027e-23]\n",
      "0.01 [4.5880027e-23]\n",
      "0.02 [3.3392473e-23]\n",
      "0.02 [4.6029213e-23]\n",
      "0.03 [2.7561957e-22]\n",
      "0.57 [4.4856218e-23]\n",
      "0.11 [3.1004468e-22]\n",
      "0.04 [4.500568e-23]\n",
      "0.06 [2.0722798e-22]\n",
      "0.07 [3.404058e-22]\n",
      "0.06 [3.289158e-23]\n",
      "0.91 [3.426948e-23]\n",
      "0.02 [8.8544424e-23]\n",
      "0.04 [5.6751834e-23]\n",
      "0.82 [3.767415e-23]\n",
      "0.25 [6.338954e-23]\n",
      "0.05 [6.92252e-23]\n",
      "0.07 [4.676834e-23]\n",
      "0.68 [2.784591e-23]\n",
      "0.06 [3.3672354e-23]\n",
      "1.0 [8.0600136e-20]\n",
      "0.13 [6.544748e-21]\n",
      "0.04 [4.711268e-23]\n",
      "0.02 [6.375354e-23]\n",
      "0.06 [2.928877e-23]\n",
      "0.11 [3.0088057e-23]\n",
      "0.9 [4.0677856e-23]\n",
      "0.09 [3.373342e-23]\n",
      "0.06 [4.7234684e-23]\n",
      "0.05 [1.2816816e-22]\n",
      "0.01 [1.03604264e-22]\n",
      "0.12 [3.459747e-23]\n",
      "1.0 [9.247996e-23]\n",
      "0.02 [7.3017716e-23]\n",
      "0.0 [8.8419885e-23]\n",
      "0.01 [3.6078336e-23]\n",
      "0.06 [6.4144344e-23]\n",
      "0.04 [1.5713917e-22]\n",
      "0.04 [4.01788e-23]\n",
      "1.0 [2.7668447e-23]\n",
      "0.76 [6.57329e-23]\n",
      "0.14 [6.0206104e-23]\n",
      "0.28 [8.4147136e-23]\n",
      "0.06 [3.589192e-23]\n",
      "0.05 [1.664133e-22]\n",
      "0.42 [4.6229818e-23]\n",
      "0.46 [3.6485517e-23]\n",
      "0.04 [5.65979e-23]\n",
      "0.02 [8.2771477e-23]\n",
      "0.07 [4.8080574e-23]\n",
      "0.12 [4.093751e-23]\n",
      "0.02 [1.2001125e-22]\n",
      "0.0 [1.3257709e-22]\n",
      "0.02 [3.826933e-23]\n",
      "0.0 [1.908513e-22]\n",
      "0.1 [1.4283201e-22]\n",
      "0.13 [9.884404e-23]\n",
      "0.12 [1.0106423e-22]\n",
      "0.0 [6.2096404e-23]\n",
      "0.02 [4.588528e-23]\n",
      "0.02 [5.7380076e-23]\n",
      "0.07 [3.7668686e-23]\n",
      "0.13 [5.2709177e-23]\n",
      "0.03 [3.3215373e-23]\n",
      "0.02 [1.2180871e-21]\n",
      "0.06 [5.338443e-23]\n",
      "0.31 [4.5824236e-23]\n",
      "0.01 [3.5036822e-23]\n",
      "0.01 [3.6482616e-22]\n",
      "0.02 [6.6880454e-23]\n",
      "1.0 [3.037938e-23]\n",
      "0.18 [3.197717e-23]\n",
      "0.06 [2.4429906e-22]\n",
      "0.02 [4.28683e-22]\n",
      "0.04 [4.8936786e-22]\n",
      "0.03 [3.6853688e-23]\n",
      "0.0 [4.7818643e-23]\n",
      "0.11 [4.9076406e-23]\n",
      "0.01 [4.8782063e-23]\n",
      "1.0 [5.2028762e-23]\n",
      "0.16 [3.5884118e-23]\n",
      "0.04 [1.1649784e-22]\n",
      "0.12 [4.0887882e-23]\n",
      "0.02 [3.3280057e-23]\n",
      "0.0 [3.356514e-23]\n",
      "0.14 [4.754762e-23]\n",
      "0.03 [2.9195846e-23]\n",
      "0.07 [4.5343955e-23]\n",
      "0.14 [5.4060505e-23]\n",
      "0.02 [3.932524e-23]\n",
      "0.25 [7.5238473e-23]\n",
      "0.03 [3.6824175e-23]\n",
      "0.12 [1.1399121e-22]\n",
      "0.0 [4.076205e-23]\n",
      "0.01 [2.4377405e-22]\n",
      "0.09 [4.7284083e-23]\n",
      "0.0 [5.104485e-22]\n",
      "0.04 [9.396929e-23]\n",
      "0.33 [6.550711e-23]\n",
      "0.0 [3.4100346e-23]\n",
      "0.41 [2.8799292e-23]\n",
      "0.05 [5.667935e-23]\n",
      "0.05 [8.3858413e-23]\n",
      "0.05 [1.2148289e-22]\n",
      "0.0 [5.723404e-23]\n",
      "0.03 [6.7347176e-23]\n",
      "0.1 [2.1159911e-22]\n",
      "0.05 [6.9987254e-23]\n",
      "0.34 [7.432505e-23]\n",
      "0.0 [4.2825785e-23]\n",
      "0.19 [5.929327e-23]\n",
      "0.04 [1.835518e-22]\n",
      "0.01 [9.322698e-23]\n",
      "0.03 [9.985745e-23]\n",
      "0.0 [4.0952818e-23]\n",
      "0.03 [1.1242968e-21]\n",
      "0.01 [2.1070424e-22]\n",
      "0.06 [8.621725e-23]\n",
      "0.02 [2.9551282e-22]\n",
      "0.2 [1.6918683e-22]\n",
      "0.03 [3.4453383e-23]\n",
      "-0.01 [4.148775e-23]\n",
      "0.03 [1.6291282e-22]\n",
      "0.01 [9.155823e-22]\n",
      "0.14 [4.6683318e-23]\n",
      "0.01 [3.0120555e-23]\n",
      "0.06 [3.8472936e-23]\n",
      "0.19 [2.820146e-23]\n",
      "0.89 [4.717184e-23]\n",
      "0.17 [2.946617e-23]\n",
      "0.01 [3.9204068e-23]\n",
      "0.03 [5.936751e-23]\n",
      "0.0 [6.118644e-22]\n",
      "0.08 [3.04634e-23]\n",
      "0.07 [5.83827e-23]\n",
      "-0.01 [4.8555376e-23]\n",
      "0.05 [8.0529635e-23]\n",
      "0.19 [5.1969854e-23]\n",
      "0.02 [1.9150621e-22]\n",
      "0.09 [5.0360722e-23]\n",
      "0.08 [1.4570636e-22]\n",
      "1.0 [5.0899382e-23]\n",
      "0.01 [2.0349833e-22]\n",
      "0.06 [7.1781e-23]\n",
      "0.02 [4.738231e-23]\n",
      "0.01 [3.6840473e-23]\n",
      "0.3 [2.8310867e-23]\n",
      "0.48 [3.6748397e-23]\n",
      "0.01 [2.7515314e-22]\n",
      "0.09 [1.1101893e-22]\n",
      "1.0 [5.732669e-23]\n",
      "0.01 [2.0055913e-21]\n",
      "0.03 [7.9648244e-23]\n",
      "0.02 [6.416563e-23]\n",
      "0.12 [5.2124915e-23]\n",
      "0.0 [1.0219939e-22]\n",
      "0.04 [2.0401776e-20]\n",
      "0.0 [5.464332e-23]\n",
      "0.02 [3.2792983e-23]\n",
      "0.31 [6.901877e-22]\n",
      "0.2 [3.6401412e-23]\n",
      "0.02 [8.9842106e-23]\n",
      "0.12 [4.693133e-23]\n",
      "0.14 [3.7929095e-23]\n",
      "0.0 [7.5435626e-23]\n",
      "0.02 [1.6713537e-22]\n",
      "0.05 [3.8749562e-22]\n",
      "0.06 [1.2857118e-22]\n",
      "0.07 [4.5018553e-23]\n",
      "0.07 [3.5310134e-23]\n",
      "0.04 [1.2694925e-22]\n",
      "0.0 [1.1477575e-22]\n",
      "0.06 [1.8720275e-22]\n",
      "0.03 [2.2701988e-20]\n",
      "0.02 [4.005943e-23]\n",
      "0.02 [4.2121963e-23]\n",
      "0.02 [9.398434e-23]\n",
      "0.03 [3.3378718e-23]\n",
      "0.03 [1.2700835e-22]\n",
      "0.31 [3.4794676e-23]\n",
      "-0.01 [5.1121213e-23]\n",
      "0.04 [2.1396765e-22]\n",
      "0.26 [1.0940538e-22]\n",
      "0.18 [3.1438101e-22]\n",
      "0.17 [4.718732e-23]\n",
      "0.0 [7.0142043e-22]\n",
      "1.0 [6.4626717e-23]\n",
      "0.24 [6.7384694e-23]\n",
      "0.07 [5.4023606e-23]\n",
      "0.05 [6.399769e-23]\n",
      "0.02 [5.0748738e-23]\n",
      "0.0 [1.247672e-22]\n",
      "0.0 [1.953165e-18]\n",
      "0.33 [3.618157e-23]\n",
      "0.08 [6.567074e-23]\n",
      "0.04 [1.8517499e-22]\n",
      "0.03 [3.217208e-23]\n",
      "0.06 [5.732757e-23]\n",
      "0.05 [7.746291e-23]\n",
      "0.23 [9.1578787e-23]\n",
      "0.11 [4.9852292e-23]\n",
      "0.46 [1.283399e-22]\n",
      "0.68 [5.631189e-16]\n",
      "0.02 [3.1138867e-23]\n",
      "0.1 [2.634759e-23]\n",
      "0.05 [9.701804e-23]\n",
      "0.12 [4.9295937e-23]\n",
      "0.28 [4.3512144e-23]\n",
      "0.0 [2.119473e-22]\n",
      "0.13 [5.8237233e-23]\n",
      "0.07 [3.4187222e-23]\n",
      "0.25 [3.6846377e-23]\n",
      "1.0 [3.090279e-23]\n",
      "0.02 [4.0131774e-23]\n",
      "0.04 [3.8533744e-23]\n",
      "0.04 [1.5836742e-22]\n",
      "0.03 [3.547945e-23]\n",
      "0.07 [1.8676123e-22]\n",
      "0.49 [5.533998e-23]\n",
      "0.07 [9.1959674e-23]\n",
      "0.01 [3.2297878e-23]\n",
      "0.01 [1.1146239e-22]\n",
      "0.08 [4.177536e-23]\n",
      "0.02 [2.8875207e-22]\n",
      "0.01 [3.6114276e-23]\n",
      "0.37 [4.1054802e-23]\n",
      "0.03 [3.9174167e-23]\n",
      "0.13 [2.7074356e-23]\n",
      "0.26 [3.107218e-23]\n",
      "0.03 [6.568402e-23]\n",
      "0.02 [6.2673024e-23]\n",
      "0.09 [4.8720138e-23]\n",
      "0.1 [9.7839786e-23]\n",
      "0.01 [5.0696105e-23]\n",
      "0.08 [5.4742636e-23]\n",
      "0.05 [5.1198496e-23]\n",
      "0.17 [1.8107528e-22]\n",
      "0.62 [1.1524739e-22]\n",
      "0.05 [6.939494e-23]\n",
      "0.23 [7.160273e-22]\n",
      "0.0 [1.888078e-22]\n",
      "0.2 [5.367993e-23]\n",
      "0.15 [5.3756156e-23]\n",
      "0.02 [8.415997e-23]\n",
      "0.01 [7.266149e-23]\n",
      "0.0 [3.8644463e-22]\n",
      "1.0 [4.466072e-23]\n",
      "0.02 [4.433667e-23]\n",
      "-0.02 [3.6357006e-23]\n",
      "0.01 [6.016914e-23]\n",
      "0.04 [1.409452e-22]\n",
      "0.04 [8.9064775e-23]\n",
      "0.05 [3.2262185e-22]\n",
      "0.01 [1.2876652e-22]\n",
      "1.0 [7.76114e-23]\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(predictData[1])):\n",
    "    print(predictData[1][index], result[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.5893358e-22],\n",
       "       [3.9083568e-23],\n",
       "       [5.8721310e-23],\n",
       "       ...,\n",
       "       [3.2262185e-22],\n",
       "       [1.2876652e-22],\n",
       "       [7.7611399e-23]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.21 [0.0155145]\n",
      "0.53 [0.0155145]\n",
      "0.49 [0.0155145]\n",
      "0.29 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.21 [0.0155145]\n",
      "0.79 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.42 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.36 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.63 [0.0155145]\n",
      "0.51 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.77 [0.0155145]\n",
      "0.83 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.39 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.27 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.32 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.17 [0.01518357]\n",
      "0.76 [0.0155145]\n",
      "0.29 [0.0155145]\n",
      "0.25 [0.0155145]\n",
      "0.46 [0.0155145]\n",
      "0.25 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.41 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.92 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.34 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.46 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.22 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.9 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.37 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.56 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.48 [0.0155145]\n",
      "0.31 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.33 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.35 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.25 [0.0155145]\n",
      "0.63 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.56 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.4 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.29 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.24 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.22 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.83 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.34 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.53 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.32 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.26 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.38 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.39 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.38 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.48 [0.0155145]\n",
      "0.21 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.2 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.24 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.21 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.24 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.5 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.39 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.71 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.2 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.3 [0.0155145]\n",
      "0.29 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.31 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.85 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.47 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.21 [0.0155145]\n",
      "0.64 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.5 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.2 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.45 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.04 [0.01520642]\n",
      "0.41 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.34 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.35 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.26 [0.0155145]\n",
      "0.72 [0.0155145]\n",
      "0.53 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.32 [0.0155145]\n",
      "0.2 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.44 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.45 [0.0155145]\n",
      "0.31 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.7 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.32 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.58 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.71 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.53 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.89 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.59 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.88 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.29 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.48 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.3 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.4 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.37 [0.0155145]\n",
      "0.21 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.33 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.83 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.32 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "0.57 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.61 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.21 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.65 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.27 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.22 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.98 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.86 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.37 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.53 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.27 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.22 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.35 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "1.0 [0.0151186]\n",
      "0.22 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.52 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "-0.01 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.81 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.22 [0.0155145]\n",
      "0.49 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.2 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.47 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.32 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.22 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.56 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.7 [0.0155145]\n",
      "0.24 [0.0155145]\n",
      "0.3 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.3 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.51 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.67 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.58 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.75 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.26 [0.0155145]\n",
      "0.54 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.21 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.52 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.51 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.43 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.49 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.24 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.33 [0.0155145]\n",
      "0.71 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.29 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.24 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.46 [0.0155145]\n",
      "0.71 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.69 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.75 [0.0155145]\n",
      "0.33 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.42 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.78 [0.0155145]\n",
      "0.37 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.91 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.33 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.31 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.53 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.97 [0.0155145]\n",
      "0.78 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.56 [0.0155145]\n",
      "0.25 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.04 [0.01514299]\n",
      "0.0 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.57 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.91 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.82 [0.0155145]\n",
      "0.25 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.68 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.9 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.76 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.42 [0.0155145]\n",
      "0.46 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.31 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.16 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.25 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.33 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.41 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.34 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.2 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "-0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.89 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "-0.01 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.19 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.3 [0.0155145]\n",
      "0.48 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.31 [0.0155145]\n",
      "0.2 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.14 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.31 [0.0155145]\n",
      "-0.01 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.26 [0.0155145]\n",
      "0.18 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.24 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.33 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.06 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "0.11 [0.0155145]\n",
      "0.46 [0.0155145]\n",
      "0.68 [0.01551064]\n",
      "0.02 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.12 [0.0155145]\n",
      "0.28 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.25 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.49 [0.0155145]\n",
      "0.07 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.37 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.13 [0.0155145]\n",
      "0.26 [0.0155145]\n",
      "0.03 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.09 [0.0155145]\n",
      "0.1 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.08 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.17 [0.0155145]\n",
      "0.62 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.23 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "0.2 [0.0155145]\n",
      "0.15 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.0 [0.0155145]\n",
      "1.0 [0.0155145]\n",
      "0.02 [0.0155145]\n",
      "-0.02 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.04 [0.0155145]\n",
      "0.05 [0.0155145]\n",
      "0.01 [0.0155145]\n",
      "1.0 [0.0155145]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(predictData[0])\n",
    "for index in range(len(predictData[1])):\n",
    "    print(predictData[1][index], result[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intend to predict whether the answer score is greater than 0 or not(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "20/20 [==============================] - 67s 3s/step - loss: 0.4804 - acc: 0.8562 - val_loss: 0.8480 - val_acc: 0.6274\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62744, saving model to ./checkpoint-01-0.6274.hdf5\n",
      "Epoch 2/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.3630 - acc: 0.8860 - val_loss: 0.4011 - val_acc: 0.8618\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.62744 to 0.86182, saving model to ./checkpoint-02-0.8618.hdf5\n",
      "Epoch 3/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.3266 - acc: 0.9027 - val_loss: 0.2837 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86182 to 0.91846, saving model to ./checkpoint-03-0.9185.hdf5\n",
      "Epoch 4/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.2929 - acc: 0.9173 - val_loss: 0.2657 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.91846 to 0.92529, saving model to ./checkpoint-04-0.9253.hdf5\n",
      "Epoch 5/24\n",
      "20/20 [==============================] - 66s 3s/step - loss: 0.2906 - acc: 0.9171 - val_loss: 0.2147 - val_acc: 0.9463\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.92529 to 0.94629, saving model to ./checkpoint-05-0.9463.hdf5\n",
      "Epoch 6/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.2570 - acc: 0.9309 - val_loss: 0.2005 - val_acc: 0.9502\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.94629 to 0.95020, saving model to ./checkpoint-06-0.9502.hdf5\n",
      "Epoch 7/24\n",
      "20/20 [==============================] - 66s 3s/step - loss: 0.2517 - acc: 0.9330 - val_loss: 0.1570 - val_acc: 0.9658\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.95020 to 0.96582, saving model to ./checkpoint-07-0.9658.hdf5\n",
      "Epoch 8/24\n",
      "20/20 [==============================] - 66s 3s/step - loss: 0.2240 - acc: 0.9425 - val_loss: 0.1723 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.2111 - acc: 0.9474 - val_loss: 0.1667 - val_acc: 0.9614\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/24\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.2063 - acc: 0.9491 - val_loss: 0.1584 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.1895 - acc: 0.9543 - val_loss: 0.1327 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.96582 to 0.97168, saving model to ./checkpoint-11-0.9717.hdf5\n",
      "Epoch 12/24\n",
      "20/20 [==============================] - 67s 3s/step - loss: 0.1905 - acc: 0.9546 - val_loss: 0.1287 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.97168 to 0.97314, saving model to ./checkpoint-12-0.9731.hdf5\n",
      "Epoch 13/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.1751 - acc: 0.9593 - val_loss: 0.1080 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.97314 to 0.97900, saving model to ./checkpoint-13-0.9790.hdf5\n",
      "Epoch 14/24\n",
      "20/20 [==============================] - 67s 3s/step - loss: 0.1628 - acc: 0.9630 - val_loss: 0.1102 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.1573 - acc: 0.9650 - val_loss: 0.0826 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.97900 to 0.98584, saving model to ./checkpoint-15-0.9858.hdf5\n",
      "Epoch 16/24\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.1497 - acc: 0.9669 - val_loss: 0.0832 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.98584 to 0.98633, saving model to ./checkpoint-16-0.9863.hdf5\n",
      "Epoch 17/24\n",
      "20/20 [==============================] - 66s 3s/step - loss: 0.1347 - acc: 0.9712 - val_loss: 0.0777 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.98633 to 0.98682, saving model to ./checkpoint-17-0.9868.hdf5\n",
      "Epoch 18/24\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.1469 - acc: 0.9674 - val_loss: 0.0806 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/24\n",
      "20/20 [==============================] - 66s 3s/step - loss: 0.1347 - acc: 0.9710 - val_loss: 0.0785 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.1283 - acc: 0.9729 - val_loss: 0.0591 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.98682 to 0.99072, saving model to ./checkpoint-20-0.9907.hdf5\n",
      "Epoch 21/24\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.1155 - acc: 0.9762 - val_loss: 0.0546 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.99072 to 0.99170, saving model to ./checkpoint-21-0.9917.hdf5\n",
      "Epoch 22/24\n",
      "20/20 [==============================] - 66s 3s/step - loss: 0.1133 - acc: 0.9769 - val_loss: 0.0704 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/24\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.1088 - acc: 0.9780 - val_loss: 0.0786 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/24\n",
      "20/20 [==============================] - 66s 3s/step - loss: 0.1220 - acc: 0.9748 - val_loss: 0.0670 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01f9b9c9e8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import horovod.keras as hvd\n",
    "\n",
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "batch_size = 1024\n",
    "num_classes = 10\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Enough epochs to demonstrate learning rate warmup and the reduction of\n",
    "# learning rate when training plateaues.\n",
    "epochs = 24\n",
    "\n",
    "def dataGenerator(validation = False):\n",
    "    while True:\n",
    "        questionOriginalFile = pd.read_csv(\"../Questions.csv\",encoding=\"utf-8\", iterator = True, chunksize=batch_size)\n",
    "        current_chunk_index = 0\n",
    "        for chunk in questionOriginalFile:\n",
    "    #         print(chunk[\"Body\"])\n",
    "            current_chunk_index += 1\n",
    "            if current_chunk_index % 100 < int(100 * validation_ratio) and not validation:\n",
    "                continue\n",
    "            if current_chunk_index % 100 >= int(100 * validation_ratio) and validation:\n",
    "                continue\n",
    "            chunk[\"Body\"] = chunk[\"Body\"].map(lambda x: one_hot(x, 10000))\n",
    "#             chunk[\"Score\"] = chunk[\"Score\"].map(lambda x: 1 if x > 0 else 0)\n",
    "            chunk[\"Score\"] = chunk[\"Score\"].map(lambda x: 1 if x >= 10 else 0)\n",
    "    #         print(chunk[\"Score\"])\n",
    "            body = pd.DataFrame(item for item in chunk[\"Body\"])\n",
    "            body = body.as_matrix()\n",
    "            body = np.nan_to_num(body)\n",
    "    #         chunk[\"Body\"].astype(\"int32\").as_matrix()\n",
    "            score = chunk[\"Score\"].as_matrix()\n",
    "            body = pad_sequences(body, maxlen=1000, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "            yield body, score\n",
    "            \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 100, input_length = 1000))\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "# model.add(Conv1D(64, 3, activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(LSTM(100, return_sequences = False))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Horovod: add Horovod Distributed Optimizer.\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "\n",
    "    # Horovod: average metrics among workers at the end of every epoch.\n",
    "    #\n",
    "    # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "    # TensorBoard or other metrics-based callbacks.\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "\n",
    "    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
    "    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
    "    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "#     hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n",
    "\n",
    "    # Reduce the learning rate if training plateaues.\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=10, verbose=1),\n",
    "]\n",
    "\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch:02d}-{val_acc:.4f}.hdf5', \\\n",
    "                                                    monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\n",
    "\n",
    "# print(next(dataGenerator()))\n",
    "# Train the model.\n",
    "# Horovod: the training will randomly sample 1 / N batches of training data and\n",
    "# 3 / N batches of validation data on every worker, where N is the number of workers.\n",
    "# Over-sampling of validation data helps to increase probability that every validation\n",
    "# example will be evaluated.\n",
    "model.fit_generator(dataGenerator(validation = False),\n",
    "                    steps_per_epoch = 20 // hvd.size(),\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=dataGenerator(validation = True),\n",
    "                    validation_steps = 2 // hvd.size())\n",
    "\n",
    "# # Evaluate the model on the full data set.\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataGenerator(validation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.03486204]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.0262668]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.0262668]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02846693]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626682]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n",
      "0 [0.02626681]\n",
      "0 [0.02626681]\n",
      "0 [0.02626682]\n",
      "0 [0.02626681]\n",
      "1 [0.02626682]\n"
     ]
    }
   ],
   "source": [
    "predictData = next(iterator)\n",
    "result = model.predict(predictData[0])\n",
    "for index in range(len(predictData[1])):\n",
    "    print(predictData[1][index], result[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(predictData[0], predictData[1], verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
