{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "9999\n",
      "[('3470', 'how do i transform sql columns into rows p i have a very simple problem which requires a very quick and simple solution in sql server p p i have a table with x columns i want to be able to select one row from the table and then transform the columns into rows p pre code tablea column column column code pre p sql statement to ruturn p pre code resulta value of column value of column value of column code pre hr p strong kevin strong i ve had a google search on the topic but alot of the example where overly complex for my example strong are you able to help further strong p p mario the solution i am creating has columns which stores the values to and i must work out how many columns have the value or more so i thought about creating a query to turn that into rows and then using the generated table in a subquery to say count the number of rows with column p '), ('4320', 'asp net application without business logic layer p is it acceptable to have an strong asp net strong application without the bll business logic layer as the following p ol li sql server data storage amp stored procedures li li data link layer strongly typed table adapters connecting to stored procs li li presentation layer aspx pages with code behind and objectdatasource for connection straight to the dll li ol p is a bll always preferable even if business logic is entirely validatable in the presentation s code behind what are the potential drawbacks for not using a bll p ')]\n",
      "[('3470', 'how'), ('3470', 'do')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf \n",
    "import pandas as pd\n",
    "import re\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/anaconda3/envs/tensorflow/bin/python3.5\"\n",
    "os.environ[\"PYTHONHASHSEED\"]=\"123\"\n",
    "#data1 = pd.read_csv(\"_Answers.csv\",encoding=\"latin-1\", iterator = True, chunksize=100)\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "questionFile=sc.textFile(\"test_q.csv\")\n",
    "answerFile=sc.textFile(\"test_a.csv\")\n",
    "tagFile = sc.textFile(\"test_t.csv\")\n",
    "stopWordsFile = sc.textFile(\"stopwords.txt\")\n",
    "print(\"finished\")\n",
    "\n",
    "questionWithHeader = questionFile.map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n",
    "header = questionWithHeader.first() #extract header\n",
    "question = questionWithHeader.filter(lambda x: x != header)\n",
    "print(question.count())\n",
    "\n",
    "questionsample=question.sample(False,0.05,81)\n",
    "questionLower = questionsample.map(lambda x: (x[0], (x[5]+x[6]).lower()))\n",
    "def f(x): return re.sub('[^a-zA-Z]+', ' ', x)\n",
    "questionLower = questionLower.mapValues(f)\n",
    "print(questionLower.take(2))\n",
    "def f2(x): return x.split(\" \")\n",
    "questionPairRaw = questionLower.flatMapValues(f2)\n",
    "print(questionPairRaw.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustring\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON']=\"/anaconda3/envs/tensorflow/bin/python3.5\"\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#Constant\n",
    "num_of_stop_words=50\n",
    "num_topics=3\n",
    "\n",
    "def document_vector(document):\n",
    "    id = document[1]\n",
    "    counts = defaultdict(int)\n",
    "    for token in document[0]:\n",
    "        if token in vocabulary:\n",
    "            token_id = vocabulary[token]\n",
    "            counts[token_id] += 1\n",
    "    counts = sorted(counts.items())\n",
    "    keys = [x[0] for x in counts]\n",
    "    values = [x[1] for x in counts]\n",
    "    return (id, Vectors.sparse(len(vocabulary), keys, values))\n",
    "\n",
    "\n",
    "data=questionPairRaw\n",
    "data = data.map(lambda words: words[1])#.filter(lambda x: len(x)>3)  \n",
    "data = data.map(lambda words: (words,1))\n",
    "data = data.reduceByKey(add) \n",
    "data=data.map(lambda tuple: (tuple[1], tuple[0])).sortByKey(False)\n",
    "\n",
    "threshold=data.take(num_of_stop_words)[num_of_stop_words - 1][0]\n",
    "\n",
    "vocabulary = data.filter(lambda x: x[0] < threshold)\n",
    "vocabulary = vocabulary.map(lambda x:x[1]).zipWithIndex().collectAsMap()\n",
    "\n",
    "inv_voc = {value: key for (key, value) in vocabulary.items()}\n",
    "\n",
    "documents=questionLower#.map(lambda doc:[x for x in doc if len(x[1])>3])\n",
    "documents=documents.map(lambda tuple: (tuple[1], tuple[0]))\n",
    "\n",
    "documents=documents.zipWithIndex()\n",
    "documents=documents.map(document_vector)\n",
    "documents=documents.map(list)\n",
    "print(documents.take(10))\n",
    "#print(vocabulary)\n",
    "#all docs\n",
    "\n",
    "\n",
    "\n",
    "with open(\"output.txt\", 'w') as f:\n",
    "    lda_model = LDA.train(documents, k=num_topics, maxIterations=100)\n",
    "                    \n",
    "    topic_indices = lda_model.describeTopics(maxTermsPerTopic=100)\n",
    "    \n",
    "    # Output topics. Each is a distribution over words (matching word count vectors)\n",
    "    print(\"Learned topics (as distributions over vocab of \" + str(lda_model.vocabSize())\\\n",
    "      + \" words):\")\n",
    "    topics = lda_model.topicsMatrix()\n",
    "    for topic in range(3):\n",
    "        print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, lda_model.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))\n",
    "\n",
    "        \n",
    "    # Print topics, showing the top-weighted 10 terms for each topic\n",
    "    for i in range(len(topic_indices)):\n",
    "        f.write(\"Topic #{0}\\n\".format(i + 1))\n",
    "        for j in range(len(topic_indices[i][0])):\n",
    "            f.write(\"{0}\\t{1}\\n\".format(inv_voc[topic_indices[i][0][j]] \\\n",
    "                .encode('utf-8'), topic_indices[i][1][j]))\n",
    "            \n",
    "\n",
    "    f.write(\"{0} topics distributed over {1} documents and {2} unique words\\n\"  \\\n",
    "        .format(num_topics, documents.count(), len(vocabulary)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['92', '61', '2008-08-01T14:45:37Z', '90', '13', '\"<p><a href=\"\"http://svnbook.red-bean.com/\"\">Version Control with Subversion</a></p>    <p>A very good resource for source control in general. Not really TortoiseSVN specific; though.</p>\"', 'False'], ['124', '26', '2008-08-01T16:09:47Z', '80', '12', '\"<p>I wound up using this. It is a kind of a hack; but it actually works pretty well. The only thing is you have to be very careful with your semicolons. : D</p>  <pre><code>var strSql:String = stream.readUTFBytes(stream.bytesAvailable);       var i:Number = 0; var strSqlSplit:Array = strSql.split(\"\";\"\"); for (i = 0; i &lt; strSqlSplit.length; i++){     NonQuery(strSqlSplit[i].toString()); } </code></pre> \"', 'True']]\n",
      "[('92', '\"<p><a href=\"\"http://svnbook.red-bean.com/\"\">Version Control with Subversion</a></p>    <p>A very good resource for source control in general. Not really TortoiseSVN specific; though.</p>\"'), ('124', '\"<p>I wound up using this. It is a kind of a hack; but it actually works pretty well. The only thing is you have to be very careful with your semicolons. : D</p>  <pre><code>var strSql:String = stream.readUTFBytes(stream.bytesAvailable);       var i:Number = 0; var strSqlSplit:Array = strSql.split(\"\";\"\"); for (i = 0; i &lt; strSqlSplit.length; i++){     NonQuery(strSqlSplit[i].toString()); } </code></pre> \"')]\n",
      "[('92', ' p a href http svnbook red bean com Version Control with Subversion a p p A very good resource for source control in general Not really TortoiseSVN specific though p '), ('124', ' p I wound up using this It is a kind of a hack but it actually works pretty well The only thing is you have to be very careful with your semicolons D p pre code var strSql String stream readUTFBytes stream bytesAvailable var i Number var strSqlSplit Array strSql split for i i lt strSqlSplit length i NonQuery strSqlSplit i toString code pre ')]\n",
      "[('92', 'Answer@92 p a href http svnbook red bean com Version Control with Subversion a p p A very good resource for source control in general Not really TortoiseSVN specific though p '), ('124', 'Answer@124 p I wound up using this It is a kind of a hack but it actually works pretty well The only thing is you have to be very careful with your semicolons D p pre code var strSql String stream readUTFBytes stream bytesAvailable var i Number var strSqlSplit Array strSql split for i i lt strSqlSplit length i NonQuery strSqlSplit i toString code pre ')]\n",
      "[('72245', 'Answer@72245 p So I googled a href http www google com search q greasemonkey dom ready greasemonkey dom ready a and the a href http www sitepoint com article beat website greasemonkey first result a seemed to say that the greasemonkey script is actually running at DOM ready so you just need to remove the onload call and run the script straight away p p I removed the em code window addEventListener load function code em and em code false code em wrapping and it worked perfectly It s strong much strong more responsive this way the page appears straight away with your script applied to it and all the unseen questions highlighted no flicker at all And there was much rejoicing yea p '), ('75904', 'Answer@75904 p akdom has given a fairly good answer but doesn t do its relevance justice p p Many common desktops including Gnome KDE and XFCE where relevant implement the specifications laid out by a href http www freedesktop org rel nofollow freedesktop org a Among these is the a href http standards freedesktop org desktop entry spec latest rel nofollow Desktop Entry Specification a which describes the format of files that define desktop icons and a href http standards freedesktop org basedir spec latest rel nofollow Desktop Base Directory Specification a that describes the locations that desktop environments should look to find these files p p Your RPM needs to include a desktop file as specified by the a href http standards freedesktop org desktop entry spec latest rel nofollow Desktop Entry Specification a and install it in the correct location as specified either by the a href http standards freedesktop org basedir spec latest rel nofollow Desktop Base Directory Specification a or in a distribution specific location I imagine there will be aliases to use in the spec file for this location p ')]\n"
     ]
    }
   ],
   "source": [
    "answerFileWithHeader = answerFile.map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n",
    "\n",
    "header = answerFileWithHeader.first() \n",
    "answer = answerFileWithHeader.filter(lambda x: x != header)\n",
    "print(answer.take(2))\n",
    "\n",
    "answer=answer.map(lambda x: (x[0],x[5]))\n",
    "print(answer.take(2))\n",
    "\n",
    "def f(x): return re.sub('[^a-zA-Z]+', ' ', x)\n",
    "answerLower = answer.mapValues(f)\n",
    "print(answerLower.take(2))\n",
    "\n",
    "answerwithTag = answerLower.map(lambda x:(x[0],\"Answer@\"+x[0]+x[1]))\n",
    "print(answerwithTag.take(2))\n",
    "\n",
    "#(questionid, sum of answers)\n",
    "def sumAsList(x):\n",
    "    list=[]\n",
    "    list.append(x[0])\n",
    "    list.append(x[1])\n",
    "    return list\n",
    "answerSum=answerwithTag.reduceByKey(sumAsList)\n",
    "print(answerSum.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
